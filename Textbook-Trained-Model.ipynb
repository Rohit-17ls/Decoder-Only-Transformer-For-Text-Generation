{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b311f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model as Model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edb25abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Textbook-text-data.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65230282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3180566"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f25a3a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n !\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_abcdefghijklmnopqrstuvwxyz{|}¬´·×âîŁžˆΔαβλπτ–—‘’“”•…′→⇐∈∑−∕∖∗√≈≠≤≥≶⋅◦⟨⟩𝖺𝖼𝗆𝗉𝗋𝗍𝗒𝚗'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a23f849c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(chars)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db242be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch : i for i, ch in enumerate(chars)}\n",
    "itos = {i : ch for i, ch in enumerate(chars)}\n",
    "encode = lambda text : [stoi[char] for char in text]\n",
    "decode = lambda encoded_text : ''.join([itos[item] for item in encoded_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94e3c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "116bc1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2862509]), torch.Size([318057]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and test split\n",
    "n = int(len(data) * 0.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "train_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4546732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    x = torch.stack([data[i : i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1 : i+block_size+1] for i in ix])\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval();\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[i] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "        \n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "381186fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64   # B\n",
    "block_size = 256  # T (Context length)\n",
    "n_layers = 6      # Number of blocks or units of the decoder in the architecture\n",
    "n_embd = 384      # num_heads * head_size\n",
    "num_heads = 6     # Number of attention heads in multiheaded attention\n",
    "head_size = n_embd // num_heads  # Sequence length processed by a single head of attention\n",
    "max_iters = 5000\n",
    "eval_iters = 200\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "dropout = 0.2      # % dropout\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# One head of self attention in multiheaded attentino\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()  # Initialize parameters for the derived class object\n",
    "        \n",
    "        self.head_size = head_size\n",
    "        self.query = nn.Linear(n_embd, head_size, bias = False) # Part of sequence being processes currently\n",
    "        self.key = nn.Linear(n_embd, head_size, bias = False)   # Parts of the sequence to attend to\n",
    "        self.value = nn.Linear(n_embd, head_size, bias = False) # Parts of sequence other than the current part\n",
    "        # tril will have no learnable parameters\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    # Forward method of a derived class of nn.Module is called by the __call__ method of the base nn.Module\n",
    "    # class. Objects of classes with __call__ method are called 'callable objects'\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape # B - batch_size, T - block_size (time dimension), C - channels\n",
    "        k = self.key(x) # (B, T, head_size)\n",
    "        q = self.key(x) # (B, T, h_s)\n",
    "        wei = (q @ k.transpose(-2, -1)) * (self.head_size**(-0.5)) # (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # Weighted aggregation of values\n",
    "        v = self.value(x)  # (B, T, h_s)\n",
    "        out = wei @ v      # (B, T, h_s)\n",
    "        return out;\n",
    "        \n",
    "        \n",
    "# Multiple heads of attention\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim = -1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "        \n",
    "        \n",
    "# Feed Forward Neural Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.network(x)\n",
    "        return out\n",
    "    \n",
    "\n",
    "# A block in the transformer decoder\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, num_heads):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadedAttention(num_heads, head_size)\n",
    "        self.feed_forward_network = FeedForward(n_embd)\n",
    "        self.layer_norm_1 = nn.LayerNorm(n_embd)\n",
    "        self.layer_norm_2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Adding x -> The purpose of the residual connection is to ensure that important\n",
    "        # information from the input sequence is preserved and propagated through the network.\n",
    "        # Also for improved gradient flow (no vanishing or exploding gradients)\n",
    "        # Original paper => Self Attention -> Add and Layer_Norm -> Feed_Forward\n",
    "        # More recently =>  Layer_Norm -> Self_Attention -> Add and Feed_Forward\n",
    "        x = x + self.self_attention(self.layer_norm_1(x)) \n",
    "        x = x + self.feed_forward_network(self.layer_norm_2(x))\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        # https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # Positional encoding\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, num_heads) for _ in range(n_layers)])\n",
    "        self.final_layer_norm = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # lanuage modelling head\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "                \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
    "            \n",
    "    \n",
    "    def forward(self, idx, targets = None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Both idx and targets have shape (B, T)\n",
    "        \n",
    "        # Forward pass through the whole decoder architecture\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = device)) # (T, C)\n",
    "        x = tok_emb + pos_emb # (B, T, C) after broadcast and add\n",
    "        x = self.blocks(x) # (B, T, C)\n",
    "        x = self.final_layer_norm(x) # (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        \n",
    "        # Loss calculation - Cross Entropy (negative log likelihood loss)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # latter block_size part of the sequence\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # compute the logits and loss\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Pick from only the current timestep\n",
    "            logits = logits[:, -1, :] # (B, T, C) to (B, C)\n",
    "            probabilites = F.softmax(logits, dim = -1) # (B, C)\n",
    "\n",
    "            # Sample from the distribution using the probabilites\n",
    "            idx_next = torch.multinomial(probabilites, num_samples = 1)  # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) # Append the sampled token(s) to the running sequence\n",
    "        \n",
    "        return idx\n",
    "            \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65848e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 : Train Loss  5.0273, Val Loss 5.0222\n",
      "loss : 5.026833534240723\n",
      "loss : 3.9979708194732666\n",
      "loss : 3.7642667293548584\n",
      "loss : 3.6232588291168213\n",
      "loss : 3.569612979888916\n",
      "loss : 3.526869058609009\n",
      "loss : 3.4389939308166504\n",
      "loss : 3.439837694168091\n",
      "loss : 3.349968433380127\n",
      "loss : 3.3302013874053955\n",
      "loss : 3.332948684692383\n",
      "loss : 3.2635531425476074\n",
      "loss : 3.235618829727173\n",
      "loss : 3.2352845668792725\n",
      "loss : 3.182049512863159\n",
      "loss : 3.2154836654663086\n",
      "loss : 3.1770501136779785\n",
      "loss : 3.139188051223755\n",
      "loss : 3.036560535430908\n",
      "loss : 3.0504915714263916\n",
      "loss : 3.0854456424713135\n",
      "loss : 2.989964723587036\n",
      "loss : 2.9857208728790283\n",
      "loss : 2.987921953201294\n",
      "loss : 2.966843843460083\n",
      "loss : 2.950360059738159\n",
      "loss : 2.9682092666625977\n",
      "loss : 2.917511224746704\n",
      "loss : 2.854088306427002\n",
      "loss : 2.9657554626464844\n",
      "loss : 2.8804538249969482\n",
      "loss : 2.8189382553100586\n",
      "loss : 2.8455111980438232\n",
      "loss : 2.834583044052124\n",
      "loss : 2.8149781227111816\n",
      "loss : 2.805117607116699\n",
      "loss : 2.845028877258301\n",
      "loss : 2.833343982696533\n",
      "loss : 2.8203768730163574\n",
      "loss : 2.774336099624634\n",
      "loss : 2.8354332447052\n",
      "loss : 2.800412178039551\n",
      "loss : 2.7086780071258545\n",
      "loss : 2.7670421600341797\n",
      "loss : 2.796740770339966\n",
      "loss : 2.7708003520965576\n",
      "loss : 2.7692694664001465\n",
      "loss : 2.7414560317993164\n",
      "loss : 2.7468864917755127\n",
      "loss : 2.7641003131866455\n",
      "loss : 2.727071762084961\n",
      "loss : 2.6912050247192383\n",
      "loss : 2.741255760192871\n",
      "loss : 2.714416980743408\n",
      "loss : 2.6836235523223877\n",
      "loss : 2.7240402698516846\n",
      "loss : 2.7312772274017334\n",
      "loss : 2.6917648315429688\n",
      "loss : 2.6445438861846924\n",
      "loss : 2.6841065883636475\n",
      "loss : 2.7023510932922363\n",
      "loss : 2.7018747329711914\n",
      "loss : 2.6660447120666504\n",
      "loss : 2.6525490283966064\n",
      "loss : 2.6584560871124268\n",
      "loss : 2.617990016937256\n",
      "loss : 2.7060964107513428\n",
      "loss : 2.6801466941833496\n",
      "loss : 2.6116690635681152\n",
      "loss : 2.652245044708252\n",
      "loss : 2.611161231994629\n",
      "loss : 2.640937566757202\n",
      "loss : 2.648477554321289\n",
      "loss : 2.6209757328033447\n",
      "loss : 2.6627821922302246\n",
      "loss : 2.6587026119232178\n",
      "loss : 2.636319160461426\n",
      "loss : 2.6127431392669678\n",
      "loss : 2.6510796546936035\n",
      "loss : 2.6220450401306152\n",
      "loss : 2.6172056198120117\n",
      "loss : 2.605163812637329\n",
      "loss : 2.613345146179199\n",
      "loss : 2.667705774307251\n",
      "loss : 2.6365513801574707\n",
      "loss : 2.6085689067840576\n",
      "loss : 2.597562789916992\n",
      "loss : 2.6217238903045654\n",
      "loss : 2.588155508041382\n",
      "loss : 2.600492000579834\n",
      "loss : 2.6056995391845703\n",
      "loss : 2.631307601928711\n",
      "loss : 2.6660714149475098\n",
      "loss : 2.581840991973877\n",
      "loss : 2.587181329727173\n",
      "loss : 2.6096582412719727\n",
      "loss : 2.567659854888916\n",
      "loss : 2.5798943042755127\n",
      "loss : 2.581234931945801\n",
      "loss : 2.5816433429718018\n",
      "loss : 2.6132333278656006\n",
      "loss : 2.6002235412597656\n",
      "loss : 2.6206581592559814\n",
      "loss : 2.581862211227417\n",
      "loss : 2.5726191997528076\n",
      "loss : 2.5729188919067383\n",
      "loss : 2.56782603263855\n",
      "loss : 2.5427591800689697\n",
      "loss : 2.5685927867889404\n",
      "loss : 2.5737762451171875\n",
      "loss : 2.5829620361328125\n",
      "loss : 2.5955874919891357\n",
      "loss : 2.589130163192749\n",
      "loss : 2.586606025695801\n",
      "loss : 2.5691025257110596\n",
      "loss : 2.611706495285034\n",
      "loss : 2.595426082611084\n",
      "loss : 2.5825893878936768\n",
      "loss : 2.576324939727783\n",
      "loss : 2.60052227973938\n",
      "loss : 2.550518274307251\n",
      "loss : 2.550314426422119\n",
      "loss : 2.5839755535125732\n",
      "loss : 2.564415454864502\n",
      "loss : 2.553034543991089\n",
      "loss : 2.5768966674804688\n",
      "loss : 2.5723745822906494\n",
      "loss : 2.5557332038879395\n",
      "loss : 2.5879969596862793\n",
      "loss : 2.5906388759613037\n",
      "loss : 2.577589988708496\n",
      "loss : 2.577047348022461\n",
      "loss : 2.6072030067443848\n",
      "loss : 2.5534276962280273\n",
      "loss : 2.5729312896728516\n",
      "loss : 2.5544989109039307\n",
      "loss : 2.610333204269409\n",
      "loss : 2.566283941268921\n",
      "loss : 2.5783815383911133\n",
      "loss : 2.5790176391601562\n",
      "loss : 2.573838949203491\n",
      "loss : 2.5518100261688232\n",
      "loss : 2.544337749481201\n",
      "loss : 2.545823335647583\n",
      "loss : 2.6026792526245117\n",
      "loss : 2.5499119758605957\n",
      "loss : 2.588097333908081\n",
      "loss : 2.6089394092559814\n",
      "loss : 2.5633816719055176\n",
      "loss : 2.567502498626709\n",
      "loss : 2.5413432121276855\n",
      "loss : 2.5859062671661377\n",
      "loss : 2.5828351974487305\n",
      "loss : 2.5652754306793213\n",
      "loss : 2.596472978591919\n",
      "loss : 2.5742013454437256\n",
      "loss : 2.541806697845459\n",
      "loss : 2.5661821365356445\n",
      "loss : 2.5657029151916504\n",
      "loss : 2.5446488857269287\n",
      "loss : 2.5916614532470703\n",
      "loss : 2.549659013748169\n",
      "loss : 2.5600147247314453\n",
      "loss : 2.5289716720581055\n",
      "loss : 2.549098014831543\n",
      "loss : 2.575833797454834\n",
      "loss : 2.5492708683013916\n",
      "loss : 2.5423622131347656\n",
      "loss : 2.5275840759277344\n",
      "loss : 2.5743043422698975\n",
      "loss : 2.5365190505981445\n",
      "loss : 2.5225751399993896\n",
      "loss : 2.5389034748077393\n",
      "loss : 2.5287461280822754\n",
      "loss : 2.5210623741149902\n",
      "loss : 2.5750572681427\n",
      "loss : 2.5435891151428223\n",
      "loss : 2.5373291969299316\n",
      "loss : 2.5369579792022705\n",
      "loss : 2.6038978099823\n",
      "loss : 2.5447192192077637\n",
      "loss : 2.545672655105591\n",
      "loss : 2.5248570442199707\n",
      "loss : 2.551353931427002\n",
      "loss : 2.5767266750335693\n",
      "loss : 2.5524821281433105\n",
      "loss : 2.533078908920288\n",
      "loss : 2.5176970958709717\n",
      "loss : 2.5525124073028564\n",
      "loss : 2.522761106491089\n",
      "loss : 2.555309772491455\n",
      "loss : 2.547668695449829\n",
      "loss : 2.5471889972686768\n",
      "loss : 2.574345111846924\n",
      "loss : 2.5503690242767334\n",
      "loss : 2.553677558898926\n",
      "loss : 2.5453574657440186\n",
      "loss : 2.5162031650543213\n",
      "loss : 2.53458833694458\n",
      "loss : 2.525656223297119\n",
      "loss : 2.536233425140381\n",
      "loss : 2.5248606204986572\n",
      "loss : 2.566453218460083\n",
      "loss : 2.5395517349243164\n",
      "loss : 2.5530285835266113\n",
      "loss : 2.532045364379883\n",
      "loss : 2.5184731483459473\n",
      "loss : 2.541370391845703\n",
      "loss : 2.5363588333129883\n",
      "loss : 2.511218547821045\n",
      "loss : 2.5172388553619385\n",
      "loss : 2.513773202896118\n",
      "loss : 2.5477757453918457\n",
      "loss : 2.547020673751831\n",
      "loss : 2.534379005432129\n",
      "loss : 2.5447702407836914\n",
      "loss : 2.5033926963806152\n",
      "loss : 2.5736682415008545\n",
      "loss : 2.553151845932007\n",
      "loss : 2.5278801918029785\n",
      "loss : 2.513498067855835\n",
      "loss : 2.5196146965026855\n",
      "loss : 2.5632896423339844\n",
      "loss : 2.539728879928589\n",
      "loss : 2.522151470184326\n",
      "loss : 2.4995906352996826\n",
      "loss : 2.5263471603393555\n",
      "loss : 2.550579309463501\n",
      "loss : 2.5519163608551025\n",
      "loss : 2.536129951477051\n",
      "loss : 2.536508560180664\n",
      "loss : 2.5295629501342773\n",
      "loss : 2.5450494289398193\n",
      "loss : 2.523237705230713\n",
      "loss : 2.543684482574463\n",
      "loss : 2.532466411590576\n",
      "loss : 2.5133113861083984\n",
      "loss : 2.5090110301971436\n",
      "loss : 2.5143275260925293\n",
      "loss : 2.5124247074127197\n",
      "loss : 2.5205776691436768\n",
      "loss : 2.5502169132232666\n",
      "loss : 2.5235443115234375\n",
      "loss : 2.550495147705078\n",
      "loss : 2.5392696857452393\n",
      "loss : 2.547664165496826\n",
      "loss : 2.542465925216675\n",
      "loss : 2.5072429180145264\n",
      "loss : 2.526914596557617\n",
      "loss : 2.517768144607544\n",
      "loss : 2.5639634132385254\n",
      "loss : 2.5249340534210205\n",
      "loss : 2.5501954555511475\n",
      "loss : 2.5167179107666016\n",
      "loss : 2.510344982147217\n",
      "loss : 2.506035566329956\n",
      "loss : 2.5206987857818604\n",
      "loss : 2.5168774127960205\n",
      "loss : 2.513953447341919\n",
      "loss : 2.546668767929077\n",
      "loss : 2.532320976257324\n",
      "loss : 2.4957542419433594\n",
      "loss : 2.531822681427002\n",
      "loss : 2.5193464756011963\n",
      "loss : 2.4887919425964355\n",
      "loss : 2.5271146297454834\n",
      "loss : 2.541260004043579\n",
      "loss : 2.488905668258667\n",
      "loss : 2.5356764793395996\n",
      "loss : 2.4975240230560303\n",
      "loss : 2.518941879272461\n",
      "loss : 2.5259974002838135\n",
      "loss : 2.508805751800537\n",
      "loss : 2.5168495178222656\n",
      "loss : 2.532506227493286\n",
      "loss : 2.537113904953003\n",
      "loss : 2.5058391094207764\n",
      "loss : 2.4967260360717773\n",
      "loss : 2.5087838172912598\n",
      "loss : 2.4974205493927\n",
      "loss : 2.533926248550415\n",
      "loss : 2.505974292755127\n",
      "loss : 2.4869046211242676\n",
      "loss : 2.498971939086914\n",
      "loss : 2.483466625213623\n",
      "loss : 2.4812088012695312\n",
      "loss : 2.474940061569214\n",
      "loss : 2.5209498405456543\n",
      "loss : 2.5030174255371094\n",
      "loss : 2.49045991897583\n",
      "loss : 2.5281524658203125\n",
      "loss : 2.546643018722534\n",
      "loss : 2.5182993412017822\n",
      "loss : 2.536699056625366\n",
      "loss : 2.513061046600342\n",
      "loss : 2.5004072189331055\n",
      "loss : 2.5264265537261963\n",
      "loss : 2.487537384033203\n",
      "loss : 2.5110816955566406\n",
      "loss : 2.490476369857788\n",
      "loss : 2.5129523277282715\n",
      "loss : 2.516831159591675\n",
      "loss : 2.517871856689453\n",
      "loss : 2.493919610977173\n",
      "loss : 2.4802989959716797\n",
      "loss : 2.485393524169922\n",
      "loss : 2.4749131202697754\n",
      "loss : 2.503816843032837\n",
      "loss : 2.4711642265319824\n",
      "loss : 2.508225917816162\n",
      "loss : 2.5187594890594482\n",
      "loss : 2.5324153900146484\n",
      "loss : 2.509521961212158\n",
      "loss : 2.497960329055786\n",
      "loss : 2.506392478942871\n",
      "loss : 2.4755032062530518\n",
      "loss : 2.4824960231781006\n",
      "loss : 2.4869840145111084\n",
      "loss : 2.5191707611083984\n",
      "loss : 2.48787260055542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 2.5067803859710693\n",
      "loss : 2.5174760818481445\n",
      "loss : 2.467745780944824\n",
      "loss : 2.4870986938476562\n",
      "loss : 2.4812729358673096\n",
      "loss : 2.467996120452881\n",
      "loss : 2.503274917602539\n",
      "loss : 2.4929769039154053\n",
      "loss : 2.4745676517486572\n",
      "loss : 2.5014805793762207\n",
      "loss : 2.493717908859253\n",
      "loss : 2.492433547973633\n",
      "loss : 2.470937728881836\n",
      "loss : 2.545311212539673\n",
      "loss : 2.4894847869873047\n",
      "loss : 2.4835734367370605\n",
      "loss : 2.4768354892730713\n",
      "loss : 2.486135482788086\n",
      "loss : 2.5146126747131348\n",
      "loss : 2.4671730995178223\n",
      "loss : 2.485496759414673\n",
      "loss : 2.4669301509857178\n",
      "loss : 2.4848053455352783\n",
      "loss : 2.514180898666382\n",
      "loss : 2.4950897693634033\n",
      "loss : 2.4798362255096436\n",
      "loss : 2.519735336303711\n",
      "loss : 2.5284423828125\n",
      "loss : 2.4899239540100098\n",
      "loss : 2.4995083808898926\n",
      "loss : 2.481398582458496\n",
      "loss : 2.474900245666504\n",
      "loss : 2.515665292739868\n",
      "loss : 2.4633235931396484\n",
      "loss : 2.481372356414795\n",
      "loss : 2.469581365585327\n",
      "loss : 2.47287917137146\n",
      "loss : 2.4733738899230957\n",
      "loss : 2.4569766521453857\n",
      "loss : 2.4686484336853027\n",
      "loss : 2.4631564617156982\n",
      "loss : 2.485201597213745\n",
      "loss : 2.455493688583374\n",
      "loss : 2.4832515716552734\n",
      "loss : 2.4418468475341797\n",
      "loss : 2.466750383377075\n",
      "loss : 2.4730641841888428\n",
      "loss : 2.4628350734710693\n",
      "loss : 2.4756839275360107\n",
      "loss : 2.478740930557251\n",
      "loss : 2.4825215339660645\n",
      "loss : 2.4714155197143555\n",
      "loss : 2.4685142040252686\n",
      "loss : 2.451554536819458\n",
      "loss : 2.4459762573242188\n",
      "loss : 2.4621338844299316\n",
      "loss : 2.494976043701172\n",
      "loss : 2.434393882751465\n",
      "loss : 2.4846384525299072\n",
      "loss : 2.478584051132202\n",
      "loss : 2.4687819480895996\n",
      "loss : 2.4710628986358643\n",
      "loss : 2.4957284927368164\n",
      "loss : 2.4402658939361572\n",
      "loss : 2.4639344215393066\n",
      "loss : 2.436129331588745\n",
      "loss : 2.455561637878418\n",
      "loss : 2.467710494995117\n",
      "loss : 2.4540305137634277\n",
      "loss : 2.4291303157806396\n",
      "loss : 2.473700523376465\n",
      "loss : 2.4700827598571777\n",
      "loss : 2.481506109237671\n",
      "loss : 2.432424783706665\n",
      "loss : 2.4707465171813965\n",
      "loss : 2.4864394664764404\n",
      "loss : 2.4627578258514404\n",
      "loss : 2.418543815612793\n",
      "loss : 2.454280138015747\n",
      "loss : 2.439424514770508\n",
      "loss : 2.4571244716644287\n",
      "loss : 2.4813060760498047\n",
      "loss : 2.4293084144592285\n",
      "loss : 2.4109835624694824\n",
      "loss : 2.455883502960205\n",
      "loss : 2.4781339168548584\n",
      "loss : 2.4473280906677246\n",
      "loss : 2.4566056728363037\n",
      "loss : 2.435262441635132\n",
      "loss : 2.460486650466919\n",
      "loss : 2.4289920330047607\n",
      "loss : 2.4220921993255615\n",
      "loss : 2.450864553451538\n",
      "loss : 2.449192523956299\n",
      "loss : 2.488356828689575\n",
      "loss : 2.4638473987579346\n",
      "loss : 2.408554792404175\n",
      "loss : 2.383972406387329\n",
      "loss : 2.430051803588867\n",
      "loss : 2.429539918899536\n",
      "loss : 2.44742751121521\n",
      "loss : 2.4242265224456787\n",
      "loss : 2.4100842475891113\n",
      "loss : 2.4312329292297363\n",
      "loss : 2.402444362640381\n",
      "loss : 2.429144859313965\n",
      "loss : 2.4188156127929688\n",
      "loss : 2.376852035522461\n",
      "loss : 2.418534755706787\n",
      "loss : 2.385124683380127\n",
      "loss : 2.419762372970581\n",
      "loss : 2.372530221939087\n",
      "loss : 2.436546802520752\n",
      "loss : 2.404123544692993\n",
      "loss : 2.430459499359131\n",
      "loss : 2.407787322998047\n",
      "loss : 2.376845121383667\n",
      "loss : 2.40058970451355\n",
      "loss : 2.3716795444488525\n",
      "loss : 2.36918306350708\n",
      "loss : 2.36985182762146\n",
      "loss : 2.377814531326294\n",
      "loss : 2.3845303058624268\n",
      "loss : 2.362109422683716\n",
      "loss : 2.36253023147583\n",
      "loss : 2.377014636993408\n",
      "loss : 2.3631041049957275\n",
      "loss : 2.387777805328369\n",
      "loss : 2.3455400466918945\n",
      "loss : 2.367126226425171\n",
      "loss : 2.351418972015381\n",
      "loss : 2.3440253734588623\n",
      "loss : 2.3623650074005127\n",
      "loss : 2.346102237701416\n",
      "loss : 2.342787742614746\n",
      "loss : 2.325688600540161\n",
      "loss : 2.3168158531188965\n",
      "loss : 2.3174831867218018\n",
      "loss : 2.312391996383667\n",
      "loss : 2.3077147006988525\n",
      "loss : 2.3071582317352295\n",
      "loss : 2.3026669025421143\n",
      "loss : 2.3276939392089844\n",
      "loss : 2.3136138916015625\n",
      "loss : 2.302309989929199\n",
      "loss : 2.3015213012695312\n",
      "loss : 2.3204455375671387\n",
      "loss : 2.260383367538452\n",
      "loss : 2.302696466445923\n",
      "loss : 2.29695463180542\n",
      "loss : 2.2633867263793945\n",
      "loss : 2.296217918395996\n",
      "loss : 2.219357490539551\n",
      "loss : 2.2362592220306396\n",
      "loss : 2.2414543628692627\n",
      "loss : 2.2681984901428223\n",
      "loss : 2.246345043182373\n",
      "loss : 2.2660157680511475\n",
      "loss : 2.214592933654785\n",
      "loss : 2.225856065750122\n",
      "loss : 2.2189395427703857\n",
      "loss : 2.2052602767944336\n",
      "loss : 2.2141077518463135\n",
      "loss : 2.23217511177063\n",
      "loss : 2.237243175506592\n",
      "loss : 2.215320587158203\n",
      "loss : 2.2166686058044434\n",
      "loss : 2.1868231296539307\n",
      "loss : 2.2268595695495605\n",
      "loss : 2.1908676624298096\n",
      "loss : 2.188284397125244\n",
      "loss : 2.1714181900024414\n",
      "loss : 2.206460475921631\n",
      "loss : 2.170668840408325\n",
      "loss : 2.2013328075408936\n",
      "loss : 2.185007333755493\n",
      "loss : 2.1784613132476807\n",
      "loss : 2.1713926792144775\n",
      "loss : 2.133183240890503\n",
      "loss : 2.1682283878326416\n",
      "Step 500 : Train Loss  2.0554, Val Loss 2.3187\n",
      "loss : 2.111490488052368\n",
      "loss : 2.1114816665649414\n",
      "loss : 2.11476993560791\n",
      "loss : 2.147850751876831\n",
      "loss : 2.1738498210906982\n",
      "loss : 2.123753070831299\n",
      "loss : 2.138505697250366\n",
      "loss : 2.163586378097534\n",
      "loss : 2.174631357192993\n",
      "loss : 2.1106462478637695\n",
      "loss : 2.109490156173706\n",
      "loss : 2.081429958343506\n",
      "loss : 2.0822932720184326\n",
      "loss : 2.0844838619232178\n",
      "loss : 2.0633301734924316\n",
      "loss : 2.084657907485962\n",
      "loss : 2.127861738204956\n",
      "loss : 2.083022356033325\n",
      "loss : 2.11932635307312\n",
      "loss : 2.0713045597076416\n",
      "loss : 2.0857200622558594\n",
      "loss : 2.0458500385284424\n",
      "loss : 2.09940505027771\n",
      "loss : 2.1332144737243652\n",
      "loss : 2.103728771209717\n",
      "loss : 2.0895233154296875\n",
      "loss : 2.0406103134155273\n",
      "loss : 1.9977262020111084\n",
      "loss : 2.054171323776245\n",
      "loss : 2.0322868824005127\n",
      "loss : 1.9982918500900269\n",
      "loss : 2.062612533569336\n",
      "loss : 2.0049591064453125\n",
      "loss : 2.026170253753662\n",
      "loss : 1.9988415241241455\n",
      "loss : 2.0454657077789307\n",
      "loss : 2.0085573196411133\n",
      "loss : 2.0140185356140137\n",
      "loss : 2.0425305366516113\n",
      "loss : 2.02052640914917\n",
      "loss : 2.0358104705810547\n",
      "loss : 2.0021235942840576\n",
      "loss : 2.011207103729248\n",
      "loss : 1.9781583547592163\n",
      "loss : 2.0315568447113037\n",
      "loss : 1.980507731437683\n",
      "loss : 2.031373977661133\n",
      "loss : 2.01580810546875\n",
      "loss : 1.9488800764083862\n",
      "loss : 1.9615349769592285\n",
      "loss : 1.948209524154663\n",
      "loss : 1.9672627449035645\n",
      "loss : 1.9507417678833008\n",
      "loss : 1.994300365447998\n",
      "loss : 1.9446401596069336\n",
      "loss : 1.9567503929138184\n",
      "loss : 1.9980766773223877\n",
      "loss : 1.9434595108032227\n",
      "loss : 1.9640058279037476\n",
      "loss : 1.9995176792144775\n",
      "loss : 1.9316409826278687\n",
      "loss : 1.8968323469161987\n",
      "loss : 1.9160327911376953\n",
      "loss : 1.9544878005981445\n",
      "loss : 1.9415113925933838\n",
      "loss : 1.941649079322815\n",
      "loss : 1.923861026763916\n",
      "loss : 1.8755393028259277\n",
      "loss : 1.9137402772903442\n",
      "loss : 1.8915797472000122\n",
      "loss : 1.9834561347961426\n",
      "loss : 1.9238455295562744\n",
      "loss : 1.905321478843689\n",
      "loss : 1.893147349357605\n",
      "loss : 1.939021348953247\n",
      "loss : 1.844519019126892\n",
      "loss : 1.8713265657424927\n",
      "loss : 1.8726904392242432\n",
      "loss : 1.874436855316162\n",
      "loss : 1.9545117616653442\n",
      "loss : 1.91472327709198\n",
      "loss : 1.860075831413269\n",
      "loss : 1.8311973810195923\n",
      "loss : 1.9083560705184937\n",
      "loss : 1.8433864116668701\n",
      "loss : 1.8651567697525024\n",
      "loss : 1.8652739524841309\n",
      "loss : 1.835017442703247\n",
      "loss : 1.869579792022705\n",
      "loss : 1.865017056465149\n",
      "loss : 1.8391072750091553\n",
      "loss : 1.8435555696487427\n",
      "loss : 1.8428988456726074\n",
      "loss : 1.8719974756240845\n",
      "loss : 1.8527157306671143\n",
      "loss : 1.8289453983306885\n",
      "loss : 1.891040563583374\n",
      "loss : 1.8345435857772827\n",
      "loss : 1.806040644645691\n",
      "loss : 1.8412975072860718\n",
      "loss : 1.8987209796905518\n",
      "loss : 1.8384391069412231\n",
      "loss : 1.798198938369751\n",
      "loss : 1.8678735494613647\n",
      "loss : 1.839454174041748\n",
      "loss : 1.8183220624923706\n",
      "loss : 1.8931834697723389\n",
      "loss : 1.8286879062652588\n",
      "loss : 1.8260703086853027\n",
      "loss : 1.8509948253631592\n",
      "loss : 1.8465569019317627\n",
      "loss : 1.8498709201812744\n",
      "loss : 1.8232932090759277\n",
      "loss : 1.857844591140747\n",
      "loss : 1.841870903968811\n",
      "loss : 1.787306547164917\n",
      "loss : 1.7772191762924194\n",
      "loss : 1.8378748893737793\n",
      "loss : 1.8039499521255493\n",
      "loss : 1.8260010480880737\n",
      "loss : 1.7641160488128662\n",
      "loss : 1.8045248985290527\n",
      "loss : 1.7913143634796143\n",
      "loss : 1.794511318206787\n",
      "loss : 1.786252498626709\n",
      "loss : 1.8057775497436523\n",
      "loss : 1.776538372039795\n",
      "loss : 1.7667231559753418\n",
      "loss : 1.763465404510498\n",
      "loss : 1.7491925954818726\n",
      "loss : 1.781044602394104\n",
      "loss : 1.789574384689331\n",
      "loss : 1.7629058361053467\n",
      "loss : 1.7789781093597412\n",
      "loss : 1.7673640251159668\n",
      "loss : 1.7478551864624023\n",
      "loss : 1.8025779724121094\n",
      "loss : 1.7669620513916016\n",
      "loss : 1.7868369817733765\n",
      "loss : 1.7593929767608643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 1.7659976482391357\n",
      "loss : 1.7695481777191162\n",
      "loss : 1.7502139806747437\n",
      "loss : 1.7483038902282715\n",
      "loss : 1.7316685914993286\n",
      "loss : 1.708818793296814\n",
      "loss : 1.7161941528320312\n",
      "loss : 1.7199416160583496\n",
      "loss : 1.7599539756774902\n",
      "loss : 1.7769269943237305\n",
      "loss : 1.731832504272461\n",
      "loss : 1.6685070991516113\n",
      "loss : 1.733532190322876\n",
      "loss : 1.6817659139633179\n",
      "loss : 1.7376316785812378\n",
      "loss : 1.7022439241409302\n",
      "loss : 1.6768996715545654\n",
      "loss : 1.7318158149719238\n",
      "loss : 1.7363942861557007\n",
      "loss : 1.7136496305465698\n",
      "loss : 1.6791332960128784\n",
      "loss : 1.7625508308410645\n",
      "loss : 1.6941344738006592\n",
      "loss : 1.7104114294052124\n",
      "loss : 1.691481113433838\n",
      "loss : 1.7563186883926392\n",
      "loss : 1.6702812910079956\n",
      "loss : 1.7108300924301147\n",
      "loss : 1.7131952047348022\n",
      "loss : 1.6825960874557495\n",
      "loss : 1.6845824718475342\n",
      "loss : 1.6787797212600708\n",
      "loss : 1.7522671222686768\n",
      "loss : 1.6807209253311157\n",
      "loss : 1.7267588376998901\n",
      "loss : 1.697048306465149\n",
      "loss : 1.691851019859314\n",
      "loss : 1.660241723060608\n",
      "loss : 1.7267003059387207\n",
      "loss : 1.711479902267456\n",
      "loss : 1.6554255485534668\n",
      "loss : 1.702172040939331\n",
      "loss : 1.7248746156692505\n",
      "loss : 1.6507041454315186\n",
      "loss : 1.655440092086792\n",
      "loss : 1.691148042678833\n",
      "loss : 1.6610428094863892\n",
      "loss : 1.634771466255188\n",
      "loss : 1.6944246292114258\n",
      "loss : 1.6238805055618286\n",
      "loss : 1.6730867624282837\n",
      "loss : 1.633091926574707\n",
      "loss : 1.685213565826416\n",
      "loss : 1.677390217781067\n",
      "loss : 1.5912365913391113\n",
      "loss : 1.673927903175354\n",
      "loss : 1.6927396059036255\n",
      "loss : 1.6952717304229736\n",
      "loss : 1.662601351737976\n",
      "loss : 1.653079867362976\n",
      "loss : 1.656567931175232\n",
      "loss : 1.6575584411621094\n",
      "loss : 1.663819432258606\n",
      "loss : 1.6703786849975586\n",
      "loss : 1.661691427230835\n",
      "loss : 1.639549970626831\n",
      "loss : 1.6271733045578003\n",
      "loss : 1.6551381349563599\n",
      "loss : 1.6524394750595093\n",
      "loss : 1.6054365634918213\n",
      "loss : 1.6478803157806396\n",
      "loss : 1.6760910749435425\n",
      "loss : 1.652050256729126\n",
      "loss : 1.6561099290847778\n",
      "loss : 1.6700055599212646\n",
      "loss : 1.6323059797286987\n",
      "loss : 1.6534391641616821\n",
      "loss : 1.5992447137832642\n",
      "loss : 1.6818809509277344\n",
      "loss : 1.6270132064819336\n",
      "loss : 1.6348624229431152\n",
      "loss : 1.6905481815338135\n",
      "loss : 1.6448088884353638\n",
      "loss : 1.5999903678894043\n",
      "loss : 1.6355444192886353\n",
      "loss : 1.627661943435669\n",
      "loss : 1.6312061548233032\n",
      "loss : 1.6019556522369385\n",
      "loss : 1.6345279216766357\n",
      "loss : 1.668134093284607\n",
      "loss : 1.620072603225708\n",
      "loss : 1.6179955005645752\n",
      "loss : 1.5747630596160889\n",
      "loss : 1.6654301881790161\n",
      "loss : 1.6087172031402588\n",
      "loss : 1.6153640747070312\n",
      "loss : 1.60695219039917\n",
      "loss : 1.6090501546859741\n",
      "loss : 1.583416223526001\n",
      "loss : 1.626112461090088\n",
      "loss : 1.58226478099823\n",
      "loss : 1.6570582389831543\n",
      "loss : 1.6162109375\n",
      "loss : 1.639164924621582\n",
      "loss : 1.6309943199157715\n",
      "loss : 1.5932384729385376\n",
      "loss : 1.5896512269973755\n",
      "loss : 1.5928518772125244\n",
      "loss : 1.634932041168213\n",
      "loss : 1.6174709796905518\n",
      "loss : 1.5908570289611816\n",
      "loss : 1.6243888139724731\n",
      "loss : 1.58237624168396\n",
      "loss : 1.6103395223617554\n",
      "loss : 1.607342004776001\n",
      "loss : 1.598462462425232\n",
      "loss : 1.5739850997924805\n",
      "loss : 1.595076084136963\n",
      "loss : 1.6513497829437256\n",
      "loss : 1.5440542697906494\n",
      "loss : 1.6225566864013672\n",
      "loss : 1.5929620265960693\n",
      "loss : 1.626556634902954\n",
      "loss : 1.555196762084961\n",
      "loss : 1.635096788406372\n",
      "loss : 1.5858203172683716\n",
      "loss : 1.566218376159668\n",
      "loss : 1.5740572214126587\n",
      "loss : 1.602216362953186\n",
      "loss : 1.574421763420105\n",
      "loss : 1.626230001449585\n",
      "loss : 1.5738328695297241\n",
      "loss : 1.6167851686477661\n",
      "loss : 1.5659890174865723\n",
      "loss : 1.618435025215149\n",
      "loss : 1.6067713499069214\n",
      "loss : 1.5612577199935913\n",
      "loss : 1.6250659227371216\n",
      "loss : 1.5798976421356201\n",
      "loss : 1.5937062501907349\n",
      "loss : 1.6149804592132568\n",
      "loss : 1.5976439714431763\n",
      "loss : 1.6398056745529175\n",
      "loss : 1.5701314210891724\n",
      "loss : 1.5696412324905396\n",
      "loss : 1.5749152898788452\n",
      "loss : 1.5607050657272339\n",
      "loss : 1.5437192916870117\n",
      "loss : 1.5892024040222168\n",
      "loss : 1.5653879642486572\n",
      "loss : 1.5663341283798218\n",
      "loss : 1.6067705154418945\n",
      "loss : 1.6173325777053833\n",
      "loss : 1.5365755558013916\n",
      "loss : 1.5403416156768799\n",
      "loss : 1.6133265495300293\n",
      "loss : 1.5527760982513428\n",
      "loss : 1.611555576324463\n",
      "loss : 1.5045946836471558\n",
      "loss : 1.4581246376037598\n",
      "loss : 1.5565851926803589\n",
      "loss : 1.54892897605896\n",
      "loss : 1.559141755104065\n",
      "loss : 1.5408780574798584\n",
      "loss : 1.5670002698898315\n",
      "loss : 1.553595781326294\n",
      "loss : 1.5647144317626953\n",
      "loss : 1.5286027193069458\n",
      "loss : 1.5143837928771973\n",
      "loss : 1.5612293481826782\n",
      "loss : 1.5644903182983398\n",
      "loss : 1.5531442165374756\n",
      "loss : 1.5486207008361816\n",
      "loss : 1.5601364374160767\n",
      "loss : 1.5398977994918823\n",
      "loss : 1.577755093574524\n",
      "loss : 1.5503371953964233\n",
      "loss : 1.4962942600250244\n",
      "loss : 1.5587247610092163\n",
      "loss : 1.5247015953063965\n",
      "loss : 1.5740141868591309\n",
      "loss : 1.5236787796020508\n",
      "loss : 1.5146414041519165\n",
      "loss : 1.509420394897461\n",
      "loss : 1.5244756937026978\n",
      "loss : 1.5153616666793823\n",
      "loss : 1.479262351989746\n",
      "loss : 1.5084691047668457\n",
      "loss : 1.532117486000061\n",
      "loss : 1.5712882280349731\n",
      "loss : 1.5290324687957764\n",
      "loss : 1.4951038360595703\n",
      "loss : 1.5336228609085083\n",
      "loss : 1.548616647720337\n",
      "loss : 1.4847590923309326\n",
      "loss : 1.5600216388702393\n",
      "loss : 1.5096789598464966\n",
      "loss : 1.5411030054092407\n",
      "loss : 1.5009841918945312\n",
      "loss : 1.4596012830734253\n",
      "loss : 1.5535434484481812\n",
      "loss : 1.4942381381988525\n",
      "loss : 1.5246068239212036\n",
      "loss : 1.5307178497314453\n",
      "loss : 1.5285868644714355\n",
      "loss : 1.5477640628814697\n",
      "loss : 1.5004518032073975\n",
      "loss : 1.5174797773361206\n",
      "loss : 1.5041801929473877\n",
      "loss : 1.5756053924560547\n",
      "loss : 1.5016138553619385\n",
      "loss : 1.507693886756897\n",
      "loss : 1.531070351600647\n",
      "loss : 1.514005422592163\n",
      "loss : 1.5548299551010132\n",
      "loss : 1.5908079147338867\n",
      "loss : 1.517051100730896\n",
      "loss : 1.5148881673812866\n",
      "loss : 1.5030546188354492\n",
      "loss : 1.5016121864318848\n",
      "loss : 1.4770326614379883\n",
      "loss : 1.5324816703796387\n",
      "loss : 1.5105634927749634\n",
      "loss : 1.5397288799285889\n",
      "loss : 1.5047959089279175\n",
      "loss : 1.5311552286148071\n",
      "loss : 1.483778953552246\n",
      "loss : 1.4663918018341064\n",
      "loss : 1.5084031820297241\n",
      "loss : 1.5186340808868408\n",
      "loss : 1.5109840631484985\n",
      "loss : 1.5277466773986816\n",
      "loss : 1.5386909246444702\n",
      "loss : 1.464767575263977\n",
      "loss : 1.5251508951187134\n",
      "loss : 1.558958888053894\n",
      "loss : 1.450273036956787\n",
      "loss : 1.4935814142227173\n",
      "loss : 1.4993629455566406\n",
      "loss : 1.5508978366851807\n",
      "loss : 1.5048062801361084\n",
      "loss : 1.5488582849502563\n",
      "loss : 1.5380723476409912\n",
      "loss : 1.5494239330291748\n",
      "loss : 1.466375708580017\n",
      "loss : 1.5211657285690308\n",
      "loss : 1.4747828245162964\n",
      "loss : 1.442469596862793\n",
      "loss : 1.4843648672103882\n",
      "loss : 1.4373462200164795\n",
      "loss : 1.508057951927185\n",
      "loss : 1.491924524307251\n",
      "loss : 1.4455374479293823\n",
      "loss : 1.4975682497024536\n",
      "loss : 1.4642987251281738\n",
      "loss : 1.5311346054077148\n",
      "loss : 1.4441437721252441\n",
      "loss : 1.4281916618347168\n",
      "loss : 1.5098906755447388\n",
      "loss : 1.4953635931015015\n",
      "loss : 1.457779884338379\n",
      "loss : 1.506941795349121\n",
      "loss : 1.4653218984603882\n",
      "loss : 1.5209399461746216\n",
      "loss : 1.4821683168411255\n",
      "loss : 1.4728132486343384\n",
      "loss : 1.4664160013198853\n",
      "loss : 1.5076898336410522\n",
      "loss : 1.5111734867095947\n",
      "loss : 1.4852824211120605\n",
      "loss : 1.4551951885223389\n",
      "loss : 1.4667404890060425\n",
      "loss : 1.4548412561416626\n",
      "loss : 1.4783380031585693\n",
      "loss : 1.4517334699630737\n",
      "loss : 1.4949712753295898\n",
      "loss : 1.4566442966461182\n",
      "loss : 1.4817452430725098\n",
      "loss : 1.4107333421707153\n",
      "loss : 1.4872119426727295\n",
      "loss : 1.4570531845092773\n",
      "loss : 1.4379609823226929\n",
      "loss : 1.5685449838638306\n",
      "loss : 1.4411489963531494\n",
      "loss : 1.4580740928649902\n",
      "loss : 1.4523062705993652\n",
      "loss : 1.4880714416503906\n",
      "loss : 1.5074775218963623\n",
      "loss : 1.5459271669387817\n",
      "loss : 1.4437910318374634\n",
      "loss : 1.4656500816345215\n",
      "loss : 1.5040442943572998\n",
      "loss : 1.4666274785995483\n",
      "loss : 1.440834641456604\n",
      "loss : 1.491186499595642\n",
      "loss : 1.4491379261016846\n",
      "loss : 1.4640650749206543\n",
      "loss : 1.4469021558761597\n",
      "loss : 1.435416579246521\n",
      "loss : 1.4779847860336304\n",
      "loss : 1.4410169124603271\n",
      "loss : 1.4949437379837036\n",
      "loss : 1.4754226207733154\n",
      "loss : 1.4299976825714111\n",
      "loss : 1.45793879032135\n",
      "loss : 1.428619384765625\n",
      "loss : 1.4607279300689697\n",
      "loss : 1.4676411151885986\n",
      "loss : 1.4948903322219849\n",
      "loss : 1.4900529384613037\n",
      "loss : 1.423417091369629\n",
      "loss : 1.4592162370681763\n",
      "loss : 1.383458137512207\n",
      "loss : 1.4328057765960693\n",
      "loss : 1.435134768486023\n",
      "loss : 1.4657800197601318\n",
      "loss : 1.438340663909912\n",
      "loss : 1.4377521276474\n",
      "loss : 1.473819613456726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 1.4410268068313599\n",
      "loss : 1.441659688949585\n",
      "loss : 1.4372210502624512\n",
      "loss : 1.4663350582122803\n",
      "loss : 1.4641351699829102\n",
      "loss : 1.5026602745056152\n",
      "loss : 1.4820610284805298\n",
      "loss : 1.4064509868621826\n",
      "loss : 1.4747952222824097\n",
      "loss : 1.4445643424987793\n",
      "loss : 1.4309203624725342\n",
      "loss : 1.4458106756210327\n",
      "loss : 1.4695823192596436\n",
      "loss : 1.459277868270874\n",
      "loss : 1.4334876537322998\n",
      "loss : 1.4281340837478638\n",
      "loss : 1.4375648498535156\n",
      "loss : 1.4347476959228516\n",
      "loss : 1.4438318014144897\n",
      "loss : 1.429190993309021\n",
      "loss : 1.456862211227417\n",
      "loss : 1.4449810981750488\n",
      "loss : 1.4180870056152344\n",
      "loss : 1.4414576292037964\n",
      "loss : 1.4388912916183472\n",
      "loss : 1.4401391744613647\n",
      "loss : 1.413459062576294\n",
      "loss : 1.4387439489364624\n",
      "loss : 1.4895180463790894\n",
      "loss : 1.4496525526046753\n",
      "loss : 1.447999119758606\n",
      "loss : 1.4099879264831543\n",
      "loss : 1.4377080202102661\n",
      "loss : 1.4368045330047607\n",
      "loss : 1.429983139038086\n",
      "loss : 1.374229073524475\n",
      "loss : 1.4373712539672852\n",
      "loss : 1.450553059577942\n",
      "loss : 1.412372350692749\n",
      "loss : 1.4168814420700073\n",
      "loss : 1.4614628553390503\n",
      "Step 1000 : Train Loss  1.3168, Val Loss 1.6831\n",
      "loss : 1.4559643268585205\n",
      "loss : 1.413130760192871\n",
      "loss : 1.4381730556488037\n",
      "loss : 1.4514578580856323\n",
      "loss : 1.3959612846374512\n",
      "loss : 1.4501311779022217\n",
      "loss : 1.429279088973999\n",
      "loss : 1.4394675493240356\n",
      "loss : 1.4507468938827515\n",
      "loss : 1.41010582447052\n",
      "loss : 1.4490649700164795\n",
      "loss : 1.465848445892334\n",
      "loss : 1.3950772285461426\n",
      "loss : 1.4360382556915283\n",
      "loss : 1.4398930072784424\n",
      "loss : 1.411000370979309\n",
      "loss : 1.4145007133483887\n",
      "loss : 1.4204541444778442\n",
      "loss : 1.4005876779556274\n",
      "loss : 1.4044151306152344\n",
      "loss : 1.4345848560333252\n",
      "loss : 1.405606985092163\n",
      "loss : 1.3760179281234741\n",
      "loss : 1.4323656558990479\n",
      "loss : 1.3957452774047852\n",
      "loss : 1.4228636026382446\n",
      "loss : 1.441589117050171\n",
      "loss : 1.418620228767395\n",
      "loss : 1.4329921007156372\n",
      "loss : 1.4064120054244995\n",
      "loss : 1.424105167388916\n",
      "loss : 1.4351577758789062\n",
      "loss : 1.4147101640701294\n",
      "loss : 1.4002281427383423\n",
      "loss : 1.4407941102981567\n",
      "loss : 1.4161394834518433\n",
      "loss : 1.3706692457199097\n",
      "loss : 1.4159432649612427\n",
      "loss : 1.4160093069076538\n",
      "loss : 1.4111078977584839\n",
      "loss : 1.377516269683838\n",
      "loss : 1.4442126750946045\n",
      "loss : 1.4385133981704712\n",
      "loss : 1.5042282342910767\n",
      "loss : 1.360634446144104\n",
      "loss : 1.3947292566299438\n",
      "loss : 1.4393666982650757\n",
      "loss : 1.393193006515503\n",
      "loss : 1.4459556341171265\n",
      "loss : 1.3999078273773193\n",
      "loss : 1.3940327167510986\n",
      "loss : 1.4256783723831177\n",
      "loss : 1.3781676292419434\n",
      "loss : 1.386486291885376\n",
      "loss : 1.4204652309417725\n",
      "loss : 1.3594509363174438\n",
      "loss : 1.420226812362671\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Setting grads to None rather than 0 for memory efficiency\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Model.GPTLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "# Using AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for iteration in range(max_iters):\n",
    "    \n",
    "    # Periodically evaluate loss and once after all training iterations are done\n",
    "    if iteration % eval_interval == 0 or iteration == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step {iteration} : Train Loss  {losses['train']:.4f}, Val Loss {losses['val']:.4f}\")\n",
    "        \n",
    "    # sample o batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # Evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    print(f\"loss : {loss}\")\n",
    "    optimizer.zero_grad(set_to_none = True)  # Setting grads to None rather than 0 for memory efficiency\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e89c82b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(m.state_dict(), './models/textbook-trained-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7981a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model.GPTLanguageModel(vocab_size)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('./models/textbook-trained-model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f87d12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTLanguageModel(\n",
       "  (token_embedding_table): Embedding(144, 384)\n",
       "  (position_embedding_table): Embedding(256, 384)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (self_attention): MultiHeadedAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (1): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (2): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (3): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (4): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (5): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed_forward_network): FeedForward(\n",
       "        (network): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (self_attention): MultiHeadedAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (1): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (2): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (3): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (4): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (5): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed_forward_network): FeedForward(\n",
       "        (network): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (self_attention): MultiHeadedAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (1): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (2): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (3): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (4): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (5): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed_forward_network): FeedForward(\n",
       "        (network): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (self_attention): MultiHeadedAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (1): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (2): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (3): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (4): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (5): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed_forward_network): FeedForward(\n",
       "        (network): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (self_attention): MultiHeadedAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (1): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (2): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (3): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (4): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (5): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed_forward_network): FeedForward(\n",
       "        (network): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (self_attention): MultiHeadedAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (1): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (2): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (3): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (4): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (5): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed_forward_network): FeedForward(\n",
       "        (network): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=384, out_features=144, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22d13992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_more(input_text):\n",
    "    encoded_context = torch.as_tensor(encode(input_text), dtype = torch.long, device = device)\n",
    "    encoded_context = torch.stack((encoded_context,)) # To turn it into a tensor of dimensions (1, context_length)\n",
    "    n = len(input_text)\n",
    "    encoded_text = model.generate(encoded_context, max_new_tokens = random.randint(20, 200))[0].tolist()\n",
    "    decoded_response = decode(encoded_text)[n:]\n",
    "    return decoded_response, len(decoded_response) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df5690fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation (106 characters) : \n",
      "ared implementations functions and evices of subsystem fthe can be extender for\n",
      "and eading to access syst\n"
     ]
    }
   ],
   "source": [
    "# Far from generating relevant stuff (nothing unexpected)\n",
    "response, size = generate_more('Processes')\n",
    "print(f'Generation ({size} characters) : {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe81f35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
