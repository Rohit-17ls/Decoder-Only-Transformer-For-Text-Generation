{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3dda408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9319ecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67b57d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chat-data.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc5bb49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607584"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc7017ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t\\n !\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~¬∞\\u200d‚Äô‚Ä¶Ô∏èüèªüèΩüëçüí´üî•üî™üî´üòÇüòÉüòÑüòÖüòÜüòäüòåüò¢üò®üò™üò≠üòÆüòØüòµüò∂üôÇüôÉüôèü•≤ü•≥\\U0001f979ü´Ç'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58500baa",
   "metadata": {},
   "source": [
    "**Slight Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28b1d917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëç\n"
     ]
    }
   ],
   "source": [
    "special_char = chars[104]\n",
    "print(special_char)\n",
    "possible_replacements = ['welcome', 'alright', 'okay']\n",
    "text = re.sub(special_char, lambda x : random.choices(possible_replacements, weights = [3, 1, 2], k = 1)[0], text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7dc6d0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t\\n !\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~¬∞\\u200d‚Äô‚Ä¶Ô∏èüèªüèΩüí´üî•üî™üî´üòÇüòÉüòÑüòÖüòÜüòäüòåüò¢üò®üò™üò≠üòÆüòØüòµüò∂üôÇüôÉüôèü•≤ü•≥\\U0001f979ü´Ç'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c46c667f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "609024"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86e5a445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‚Äô‚Ä¶Ô∏èüèªüèΩüí´üî•üî™üî´üòÇüòÉüòÑüòÖüòÜüòäüòåüò¢üò®üò™üò≠üòÆüòØüòµüò∂üôÇüôÉüôèü•≤ü•≥\\U0001f979ü´Ç'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exotic_chars =  chars[99:]\n",
    "''.join(exotic_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d531271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607855"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for exotic_char in exotic_chars:\n",
    "    text = text.replace(exotic_char, '')\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa09ab73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t\\n !\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~¬∞\\u200d'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c317f562",
   "metadata": {},
   "source": [
    "**Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "642e2afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(chars)\n",
    "batch_size = 64   # B\n",
    "block_size = 256  # T (Context length)\n",
    "n_layers = 6      # Number of blocks or units of the decoder in the architecture\n",
    "n_embd = 384      # num_heads * head_size\n",
    "num_heads = 6     # Number of attention heads in multiheaded attention\n",
    "head_size = n_embd // num_heads  # Sequence length processed by a single head of attention\n",
    "max_iters = 5000\n",
    "eval_iters = 200\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "dropout = 0.2      # % dropout\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22830651",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda text : [stoi[token] for token in text]\n",
    "decode = lambda encoding : ''.join([itos[item] for item in encoding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4030f595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54, 74, 75, 85, 2, 75, 85, 2, 67, 2, 78, 75, 80, 71, 2, 81, 72, 2, 86, 71, 90, 86]\n",
      "This is a line of text\n"
     ]
    }
   ],
   "source": [
    "e = encode(\"This is a line of text\")\n",
    "print(e, decode(e), sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ed6fef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the text dataset into a torch tensor\n",
    "data = torch.tensor(encode(text), dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d940693b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([607855])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f527aa33",
   "metadata": {},
   "source": [
    "**Train and Val splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a12eeef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(data) * 0.95)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c532afcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([577462]), torch.Size([30393]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e1890c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    x = torch.stack([data[i : i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1 : i+block_size+1] for i in ix])\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "03c48dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval();\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[i] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "        \n",
    "    model.train()\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5089619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One head of self attention in multiheaded attentino\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()  # Initialize parameters for the derived class object\n",
    "        \n",
    "        self.head_size = head_size\n",
    "        self.query = nn.Linear(n_embd, head_size, bias = False) # Part of sequence being processes currently\n",
    "        self.key = nn.Linear(n_embd, head_size, bias = False)   # Parts of the sequence to attend to\n",
    "        self.value = nn.Linear(n_embd, head_size, bias = False) # Parts of sequence other than the current part\n",
    "        # tril will have no learnable parameters\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    # Forward method of a derived class of nn.Module is called by the __call__ method of the base nn.Module\n",
    "    # class. Objects of classes with __call__ method are called 'callable objects'\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape # B - batch_size, T - block_size (time dimension), C - channels\n",
    "        k = self.key(x) # (B, T, head_size)\n",
    "        q = self.key(x) # (B, T, h_s)\n",
    "        wei = (q @ k.transpose(-2, -1)) * (self.head_size**(-0.5)) # (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # Weighted aggregation of values\n",
    "        v = self.value(x)  # (B, T, h_s)\n",
    "        out = wei @ v      # (B, T, h_s)\n",
    "        return out;\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "22c28c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple heads of attention\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim = -1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9f068ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward Neural Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.network(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "97646252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A block in the transformer decoder\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, num_heads):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadedAttention(num_heads, head_size)\n",
    "        self.feed_forward_network = FeedForward(n_embd)\n",
    "        self.layer_norm_1 = nn.LayerNorm(n_embd)\n",
    "        self.layer_norm_2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Adding x -> The purpose of the residual connection is to ensure that important\n",
    "        # information from the input sequence is preserved and propagated through the network.\n",
    "        # Also for improved gradient flow (no vanishing or exploding gradients)\n",
    "        # Original paper => Self Attention -> Add and Layer_Norm -> Feed_Forward\n",
    "        # More recently =>  Layer_Norm -> Self_Attention -> Add and Feed_Forward\n",
    "        x = x + self.self_attention(self.layer_norm_1(x)) \n",
    "        x = x + self.feed_forward_network(self.layer_norm_2(x))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c63fe85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        # https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # Positional encoding\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, num_heads) for _ in range(n_layers)])\n",
    "        self.final_layer_norm = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # lanuage modelling head\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "                \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
    "            \n",
    "    \n",
    "    def forward(self, idx, targets = None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Both idx and targets have shape (B, T)\n",
    "        \n",
    "        # Forward pass through the whole decoder architecture\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = device)) # (T, C)\n",
    "        x = tok_emb + pos_emb # (B, T, C) after broadcast and add\n",
    "        x = self.blocks(x) # (B, T, C)\n",
    "        x = self.final_layer_norm(x) # (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        \n",
    "        # Loss calculation - Cross Entropy (negative log likelihood loss)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # latter block_size part of the sequence\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # compute the logits and loss\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Pick from only the current timestep\n",
    "            logits = logits[:, -1, :] # (B, T, C) to (B, C)\n",
    "            probabilites = F.softmax(logits, dim = -1) # (B, C)\n",
    "\n",
    "            # Sample from the distribution using the probabilites\n",
    "            idx_next = torch.multinomial(probabilites, num_samples = 1)  # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) # Append the sampled token(s) to the running sequence\n",
    "        \n",
    "        return idx\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b33209ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 : Train Loss  4.5896, Val Loss 4.6097\n",
      "loss : 4.594367027282715\n",
      "loss : 3.8994452953338623\n",
      "loss : 3.6169638633728027\n",
      "loss : 3.479755401611328\n",
      "loss : 3.309957981109619\n",
      "loss : 3.358476400375366\n",
      "loss : 3.267212152481079\n",
      "loss : 3.305375576019287\n",
      "loss : 3.180311679840088\n",
      "loss : 3.191066265106201\n",
      "loss : 3.190197706222534\n",
      "loss : 3.0351381301879883\n",
      "loss : 3.178213119506836\n",
      "loss : 3.023707628250122\n",
      "loss : 3.08182954788208\n",
      "loss : 2.9676549434661865\n",
      "loss : 2.9725615978240967\n",
      "loss : 3.005405902862549\n",
      "loss : 3.0240659713745117\n",
      "loss : 3.098667860031128\n",
      "loss : 3.137862205505371\n",
      "loss : 3.021477222442627\n",
      "loss : 3.0966060161590576\n",
      "loss : 2.943950891494751\n",
      "loss : 3.0849602222442627\n",
      "loss : 3.017197370529175\n",
      "loss : 2.983914852142334\n",
      "loss : 3.027194023132324\n",
      "loss : 3.055826425552368\n",
      "loss : 2.9053826332092285\n",
      "loss : 2.8429179191589355\n",
      "loss : 2.884289503097534\n",
      "loss : 2.8416547775268555\n",
      "loss : 2.9595534801483154\n",
      "loss : 2.8305060863494873\n",
      "loss : 2.8801348209381104\n",
      "loss : 2.8043065071105957\n",
      "loss : 2.8624861240386963\n",
      "loss : 2.619418144226074\n",
      "loss : 2.9349448680877686\n",
      "loss : 2.8499462604522705\n",
      "loss : 2.900784969329834\n",
      "loss : 2.7123212814331055\n",
      "loss : 2.733203649520874\n",
      "loss : 2.831883192062378\n",
      "loss : 2.7353575229644775\n",
      "loss : 2.6465587615966797\n",
      "loss : 2.7118663787841797\n",
      "loss : 2.6945583820343018\n",
      "loss : 2.629262685775757\n",
      "loss : 2.71915864944458\n",
      "loss : 2.6402223110198975\n",
      "loss : 2.725663423538208\n",
      "loss : 2.6920692920684814\n",
      "loss : 2.6739611625671387\n",
      "loss : 2.6017374992370605\n",
      "loss : 2.58841872215271\n",
      "loss : 2.6426916122436523\n",
      "loss : 2.6310479640960693\n",
      "loss : 2.718858003616333\n",
      "loss : 2.6071035861968994\n",
      "loss : 2.5685441493988037\n",
      "loss : 2.629561185836792\n",
      "loss : 2.349803924560547\n",
      "loss : 2.5322399139404297\n",
      "loss : 2.5905890464782715\n",
      "loss : 2.5856614112854004\n",
      "loss : 2.5944013595581055\n",
      "loss : 2.5960962772369385\n",
      "loss : 2.5666637420654297\n",
      "loss : 2.4680979251861572\n",
      "loss : 2.5418758392333984\n",
      "loss : 2.5889925956726074\n",
      "loss : 2.6159770488739014\n",
      "loss : 2.6178345680236816\n",
      "loss : 2.445187568664551\n",
      "loss : 2.424149990081787\n",
      "loss : 2.4434683322906494\n",
      "loss : 2.5667123794555664\n",
      "loss : 2.5287837982177734\n",
      "loss : 2.5457193851470947\n",
      "loss : 2.5421555042266846\n",
      "loss : 2.439340114593506\n",
      "loss : 2.554711103439331\n",
      "loss : 2.510425329208374\n",
      "loss : 2.550210475921631\n",
      "loss : 2.47003173828125\n",
      "loss : 2.451387643814087\n",
      "loss : 2.5394203662872314\n",
      "loss : 2.5073211193084717\n",
      "loss : 2.5175321102142334\n",
      "loss : 2.3733866214752197\n",
      "loss : 2.3344881534576416\n",
      "loss : 2.518620252609253\n",
      "loss : 2.455056667327881\n",
      "loss : 2.484515905380249\n",
      "loss : 2.5178418159484863\n",
      "loss : 2.341982126235962\n",
      "loss : 2.4143667221069336\n",
      "loss : 2.3830623626708984\n",
      "loss : 2.477975368499756\n",
      "loss : 2.416156768798828\n",
      "loss : 2.455441951751709\n",
      "loss : 2.496182680130005\n",
      "loss : 2.452469825744629\n",
      "loss : 2.478672981262207\n",
      "loss : 2.5820915699005127\n",
      "loss : 2.3917460441589355\n",
      "loss : 2.5083391666412354\n",
      "loss : 2.5596182346343994\n",
      "loss : 2.2786402702331543\n",
      "loss : 2.3564107418060303\n",
      "loss : 2.3688526153564453\n",
      "loss : 2.4198687076568604\n",
      "loss : 2.4963886737823486\n",
      "loss : 2.4102656841278076\n",
      "loss : 2.4412333965301514\n",
      "loss : 2.4046285152435303\n",
      "loss : 2.4728915691375732\n",
      "loss : 2.3488099575042725\n",
      "loss : 2.3859338760375977\n",
      "loss : 2.4974024295806885\n",
      "loss : 2.5220489501953125\n",
      "loss : 2.486154794692993\n",
      "loss : 2.4821152687072754\n",
      "loss : 2.4560773372650146\n",
      "loss : 2.3838343620300293\n",
      "loss : 2.424574851989746\n",
      "loss : 2.4295828342437744\n",
      "loss : 2.3517041206359863\n",
      "loss : 2.3417279720306396\n",
      "loss : 2.4507248401641846\n",
      "loss : 2.468391180038452\n",
      "loss : 2.4242546558380127\n",
      "loss : 2.354203462600708\n",
      "loss : 2.5122992992401123\n",
      "loss : 2.3164620399475098\n",
      "loss : 2.3691821098327637\n",
      "loss : 2.517261028289795\n",
      "loss : 2.5583858489990234\n",
      "loss : 2.4211978912353516\n",
      "loss : 2.501713275909424\n",
      "loss : 2.484232187271118\n",
      "loss : 2.3688302040100098\n",
      "loss : 2.532146692276001\n",
      "loss : 2.4704501628875732\n",
      "loss : 2.484778881072998\n",
      "loss : 2.485302448272705\n",
      "loss : 2.3445162773132324\n",
      "loss : 2.4514870643615723\n",
      "loss : 2.4732205867767334\n",
      "loss : 2.508439302444458\n",
      "loss : 2.4352686405181885\n",
      "loss : 2.291053533554077\n",
      "loss : 2.331618309020996\n",
      "loss : 2.3245763778686523\n",
      "loss : 2.481130838394165\n",
      "loss : 2.4014720916748047\n",
      "loss : 2.3608179092407227\n",
      "loss : 2.4173707962036133\n",
      "loss : 2.4088077545166016\n",
      "loss : 2.387531042098999\n",
      "loss : 2.5317821502685547\n",
      "loss : 2.461634635925293\n",
      "loss : 2.442722797393799\n",
      "loss : 2.3622748851776123\n",
      "loss : 2.4876883029937744\n",
      "loss : 2.388084650039673\n",
      "loss : 2.502849817276001\n",
      "loss : 2.497849702835083\n",
      "loss : 2.5073304176330566\n",
      "loss : 2.4585602283477783\n",
      "loss : 2.3919334411621094\n",
      "loss : 2.471043825149536\n",
      "loss : 2.4555962085723877\n",
      "loss : 2.349151849746704\n",
      "loss : 2.451629400253296\n",
      "loss : 2.5472846031188965\n",
      "loss : 2.371584892272949\n",
      "loss : 2.416285276412964\n",
      "loss : 2.4557974338531494\n",
      "loss : 2.430983066558838\n",
      "loss : 2.4157326221466064\n",
      "loss : 2.362773895263672\n",
      "loss : 2.453939199447632\n",
      "loss : 2.4296250343322754\n",
      "loss : 2.4126205444335938\n",
      "loss : 2.5341968536376953\n",
      "loss : 2.3675198554992676\n",
      "loss : 2.362149238586426\n",
      "loss : 2.4233977794647217\n",
      "loss : 2.391284227371216\n",
      "loss : 2.452284574508667\n",
      "loss : 2.212747812271118\n",
      "loss : 2.3752281665802\n",
      "loss : 2.2821671962738037\n",
      "loss : 2.3329555988311768\n",
      "loss : 2.343282461166382\n",
      "loss : 2.3517913818359375\n",
      "loss : 2.317441701889038\n",
      "loss : 2.4590489864349365\n",
      "loss : 2.2855184078216553\n",
      "loss : 2.456721782684326\n",
      "loss : 2.250488042831421\n",
      "loss : 2.394542932510376\n",
      "loss : 2.379554271697998\n",
      "loss : 2.3528120517730713\n",
      "loss : 2.44317889213562\n",
      "loss : 2.456895351409912\n",
      "loss : 2.4786629676818848\n",
      "loss : 2.344877004623413\n",
      "loss : 2.4210593700408936\n",
      "loss : 2.4340529441833496\n",
      "loss : 2.4513237476348877\n",
      "loss : 2.4168639183044434\n",
      "loss : 2.439995050430298\n",
      "loss : 2.444211959838867\n",
      "loss : 2.41679310798645\n",
      "loss : 2.3663740158081055\n",
      "loss : 2.4201643466949463\n",
      "loss : 2.2891154289245605\n",
      "loss : 2.4773311614990234\n",
      "loss : 2.510765314102173\n",
      "loss : 2.497363805770874\n",
      "loss : 2.395559549331665\n",
      "loss : 2.4698879718780518\n",
      "loss : 2.4027981758117676\n",
      "loss : 2.397749662399292\n",
      "loss : 2.28755784034729\n",
      "loss : 2.407120704650879\n",
      "loss : 2.3765358924865723\n",
      "loss : 2.4089977741241455\n",
      "loss : 2.2974414825439453\n",
      "loss : 2.4142298698425293\n",
      "loss : 2.337765693664551\n",
      "loss : 2.3879201412200928\n",
      "loss : 2.5085697174072266\n",
      "loss : 2.3811118602752686\n",
      "loss : 2.5037295818328857\n",
      "loss : 2.370242118835449\n",
      "loss : 2.478419780731201\n",
      "loss : 2.425269365310669\n",
      "loss : 2.3916070461273193\n",
      "loss : 2.40073823928833\n",
      "loss : 2.426348924636841\n",
      "loss : 2.373610019683838\n",
      "loss : 2.3921525478363037\n",
      "loss : 2.240017890930176\n",
      "loss : 2.4952189922332764\n",
      "loss : 2.407834529876709\n",
      "loss : 2.3156661987304688\n",
      "loss : 2.3711016178131104\n",
      "loss : 2.4349639415740967\n",
      "loss : 2.2832353115081787\n",
      "loss : 2.4487931728363037\n",
      "loss : 2.40864896774292\n",
      "loss : 2.3280904293060303\n",
      "loss : 2.4060781002044678\n",
      "loss : 2.369863271713257\n",
      "loss : 2.444450855255127\n",
      "loss : 2.347771406173706\n",
      "loss : 2.4834036827087402\n",
      "loss : 2.417487859725952\n",
      "loss : 2.412400484085083\n",
      "loss : 2.468397378921509\n",
      "loss : 2.487708330154419\n",
      "loss : 2.447362184524536\n",
      "loss : 2.4506735801696777\n",
      "loss : 2.3466851711273193\n",
      "loss : 2.4275152683258057\n",
      "loss : 2.408161163330078\n",
      "loss : 2.4162704944610596\n",
      "loss : 2.327894687652588\n",
      "loss : 2.351562738418579\n",
      "loss : 2.3936355113983154\n",
      "loss : 2.32446551322937\n",
      "loss : 2.4647181034088135\n",
      "loss : 2.445690631866455\n",
      "loss : 2.4436161518096924\n",
      "loss : 2.4374897480010986\n",
      "loss : 2.4089722633361816\n",
      "loss : 2.328557252883911\n",
      "loss : 2.5142710208892822\n",
      "loss : 2.316359519958496\n",
      "loss : 2.283923864364624\n",
      "loss : 2.2818171977996826\n",
      "loss : 2.3122122287750244\n",
      "loss : 2.4870076179504395\n",
      "loss : 2.3990557193756104\n",
      "loss : 2.5076353549957275\n",
      "loss : 2.2904469966888428\n",
      "loss : 2.4296352863311768\n",
      "loss : 2.355677366256714\n",
      "loss : 2.3410980701446533\n",
      "loss : 2.415541648864746\n",
      "loss : 2.374985456466675\n",
      "loss : 2.3572137355804443\n",
      "loss : 2.3755345344543457\n",
      "loss : 2.4418861865997314\n",
      "loss : 2.282567262649536\n",
      "loss : 2.3052191734313965\n",
      "loss : 2.3196160793304443\n",
      "loss : 2.2734925746917725\n",
      "loss : 2.285740613937378\n",
      "loss : 2.4328691959381104\n",
      "loss : 2.358625888824463\n",
      "loss : 2.392263650894165\n",
      "loss : 2.3042426109313965\n",
      "loss : 2.4319305419921875\n",
      "loss : 2.279812812805176\n",
      "loss : 2.4462902545928955\n",
      "loss : 2.3660712242126465\n",
      "loss : 2.398911952972412\n",
      "loss : 2.4462132453918457\n",
      "loss : 2.307260751724243\n",
      "loss : 2.332430124282837\n",
      "loss : 2.506406307220459\n",
      "loss : 2.3972840309143066\n",
      "loss : 2.298267364501953\n",
      "loss : 2.302734613418579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 2.4092953205108643\n",
      "loss : 2.4704999923706055\n",
      "loss : 2.392650604248047\n",
      "loss : 2.4062252044677734\n",
      "loss : 2.318263292312622\n",
      "loss : 2.348052501678467\n",
      "loss : 2.273977279663086\n",
      "loss : 2.3864355087280273\n",
      "loss : 2.44482421875\n",
      "loss : 2.3584861755371094\n",
      "loss : 2.348767042160034\n",
      "loss : 2.425851583480835\n",
      "loss : 2.301772356033325\n",
      "loss : 2.338906764984131\n",
      "loss : 2.4108381271362305\n",
      "loss : 2.3891804218292236\n",
      "loss : 2.3257131576538086\n",
      "loss : 2.4690256118774414\n",
      "loss : 2.5030364990234375\n",
      "loss : 2.4199769496917725\n",
      "loss : 2.3715693950653076\n",
      "loss : 2.318317174911499\n",
      "loss : 2.2343692779541016\n",
      "loss : 2.3696553707122803\n",
      "loss : 2.4946095943450928\n",
      "loss : 2.294741630554199\n",
      "loss : 2.340785503387451\n",
      "loss : 2.430307388305664\n",
      "loss : 2.3587801456451416\n",
      "loss : 2.295630693435669\n",
      "loss : 2.42000412940979\n",
      "loss : 2.349846839904785\n",
      "loss : 2.344032049179077\n",
      "loss : 2.320509195327759\n",
      "loss : 2.437417507171631\n",
      "loss : 2.4130353927612305\n",
      "loss : 2.4545137882232666\n",
      "loss : 2.2845187187194824\n",
      "loss : 2.2783730030059814\n",
      "loss : 2.2362804412841797\n",
      "loss : 2.364351272583008\n",
      "loss : 2.3256866931915283\n",
      "loss : 2.3900363445281982\n",
      "loss : 2.353161096572876\n",
      "loss : 2.3452346324920654\n",
      "loss : 2.4295668601989746\n",
      "loss : 2.3279237747192383\n",
      "loss : 2.438946485519409\n",
      "loss : 2.3985743522644043\n",
      "loss : 2.3770928382873535\n",
      "loss : 2.4310879707336426\n",
      "loss : 2.4521286487579346\n",
      "loss : 2.4439334869384766\n",
      "loss : 2.362581491470337\n",
      "loss : 2.3370182514190674\n",
      "loss : 2.4097886085510254\n",
      "loss : 2.420503854751587\n",
      "loss : 2.464972496032715\n",
      "loss : 2.388716697692871\n",
      "loss : 2.4217164516448975\n",
      "loss : 2.356368064880371\n",
      "loss : 2.3049018383026123\n",
      "loss : 2.380577564239502\n",
      "loss : 2.387761354446411\n",
      "loss : 2.3405580520629883\n",
      "loss : 2.425865888595581\n",
      "loss : 2.414421796798706\n",
      "loss : 2.3066623210906982\n",
      "loss : 2.3437440395355225\n",
      "loss : 2.3993935585021973\n",
      "loss : 2.373775005340576\n",
      "loss : 2.3131823539733887\n",
      "loss : 2.420238971710205\n",
      "loss : 2.2922911643981934\n",
      "loss : 2.4956629276275635\n",
      "loss : 2.410031318664551\n",
      "loss : 2.408238172531128\n",
      "loss : 2.360769271850586\n",
      "loss : 2.4085853099823\n",
      "loss : 2.295621395111084\n",
      "loss : 2.4078238010406494\n",
      "loss : 2.269275665283203\n",
      "loss : 2.3124303817749023\n",
      "loss : 2.405752420425415\n",
      "loss : 2.437166213989258\n",
      "loss : 2.4603209495544434\n",
      "loss : 2.349958658218384\n",
      "loss : 2.3782804012298584\n",
      "loss : 2.3722755908966064\n",
      "loss : 2.4064412117004395\n",
      "loss : 2.3099923133850098\n",
      "loss : 2.469514846801758\n",
      "loss : 2.420693874359131\n",
      "loss : 2.368164300918579\n",
      "loss : 2.3466250896453857\n",
      "loss : 2.3166255950927734\n",
      "loss : 2.4251797199249268\n",
      "loss : 2.3488547801971436\n",
      "loss : 2.440427541732788\n",
      "loss : 2.308753252029419\n",
      "loss : 2.4701383113861084\n",
      "loss : 2.2680413722991943\n",
      "loss : 2.4115281105041504\n",
      "loss : 2.374180555343628\n",
      "loss : 2.290931224822998\n",
      "loss : 2.37033748626709\n",
      "loss : 2.3132238388061523\n",
      "loss : 2.354701042175293\n",
      "loss : 2.246563673019409\n",
      "loss : 2.4347710609436035\n",
      "loss : 2.375246524810791\n",
      "loss : 2.400449514389038\n",
      "loss : 2.178734540939331\n",
      "loss : 2.287365674972534\n",
      "loss : 2.3106281757354736\n",
      "loss : 2.253007173538208\n",
      "loss : 2.4389913082122803\n",
      "loss : 2.2497050762176514\n",
      "loss : 2.371528387069702\n",
      "loss : 2.3123700618743896\n",
      "loss : 2.340052366256714\n",
      "loss : 2.325439214706421\n",
      "loss : 2.3680784702301025\n",
      "loss : 2.4053902626037598\n",
      "loss : 2.356248140335083\n",
      "loss : 2.352869749069214\n",
      "loss : 2.4057886600494385\n",
      "loss : 2.319676160812378\n",
      "loss : 2.4026947021484375\n",
      "loss : 2.323673725128174\n",
      "loss : 2.401142120361328\n",
      "loss : 2.382953405380249\n",
      "loss : 2.380133628845215\n",
      "loss : 2.3007209300994873\n",
      "loss : 2.1536507606506348\n",
      "loss : 2.2749998569488525\n",
      "loss : 2.4255902767181396\n",
      "loss : 2.186737298965454\n",
      "loss : 2.384709358215332\n",
      "loss : 2.36387300491333\n",
      "loss : 2.3365893363952637\n",
      "loss : 2.3576889038085938\n",
      "loss : 2.3637633323669434\n",
      "loss : 2.3673038482666016\n",
      "loss : 2.4526174068450928\n",
      "loss : 2.2429311275482178\n",
      "loss : 2.3788228034973145\n",
      "loss : 2.2597241401672363\n",
      "loss : 2.251253128051758\n",
      "loss : 2.344343900680542\n",
      "loss : 2.365841865539551\n",
      "loss : 2.2754135131835938\n",
      "loss : 2.3357584476470947\n",
      "loss : 2.432039976119995\n",
      "loss : 2.2591967582702637\n",
      "loss : 2.335010290145874\n",
      "loss : 2.29329776763916\n",
      "loss : 2.2843618392944336\n",
      "loss : 2.3543481826782227\n",
      "loss : 2.267008066177368\n",
      "loss : 2.3200843334198\n",
      "loss : 2.1850907802581787\n",
      "loss : 2.3678364753723145\n",
      "loss : 2.3517472743988037\n",
      "loss : 2.240705728530884\n",
      "loss : 2.3667125701904297\n",
      "loss : 2.3579084873199463\n",
      "loss : 2.2685513496398926\n",
      "loss : 2.3398427963256836\n",
      "loss : 2.3308041095733643\n",
      "loss : 2.3374242782592773\n",
      "loss : 2.2015438079833984\n",
      "loss : 2.1963648796081543\n",
      "loss : 2.2724268436431885\n",
      "loss : 2.425198793411255\n",
      "loss : 2.415729284286499\n",
      "loss : 2.2768800258636475\n",
      "loss : 2.3024239540100098\n",
      "loss : 2.326317071914673\n",
      "loss : 2.2028391361236572\n",
      "Step 500 : Train Loss  2.2486, Val Loss 2.3629\n",
      "loss : 2.21754789352417\n",
      "loss : 2.312256336212158\n",
      "loss : 2.3680267333984375\n",
      "loss : 2.167370319366455\n",
      "loss : 2.303171157836914\n",
      "loss : 2.293043613433838\n",
      "loss : 2.373929023742676\n",
      "loss : 2.3402962684631348\n",
      "loss : 2.315049648284912\n",
      "loss : 2.1868131160736084\n",
      "loss : 2.2859623432159424\n",
      "loss : 2.310800790786743\n",
      "loss : 2.297353506088257\n",
      "loss : 2.3257644176483154\n",
      "loss : 2.3309054374694824\n",
      "loss : 2.245535373687744\n",
      "loss : 2.2487988471984863\n",
      "loss : 2.308759927749634\n",
      "loss : 2.2968406677246094\n",
      "loss : 2.253305435180664\n",
      "loss : 2.262101411819458\n",
      "loss : 2.228381633758545\n",
      "loss : 2.2832674980163574\n",
      "loss : 2.2211456298828125\n",
      "loss : 2.344372510910034\n",
      "loss : 2.2475357055664062\n",
      "loss : 2.2853951454162598\n",
      "loss : 2.4194817543029785\n",
      "loss : 2.3157882690429688\n",
      "loss : 2.210113048553467\n",
      "loss : 2.32709002494812\n",
      "loss : 2.1745245456695557\n",
      "loss : 2.1856205463409424\n",
      "loss : 2.2792885303497314\n",
      "loss : 2.3370649814605713\n",
      "loss : 2.3200597763061523\n",
      "loss : 2.2196531295776367\n",
      "loss : 2.25283145904541\n",
      "loss : 2.2420244216918945\n",
      "loss : 2.231224298477173\n",
      "loss : 2.1937499046325684\n",
      "loss : 2.1602954864501953\n",
      "loss : 2.2137227058410645\n",
      "loss : 2.1193461418151855\n",
      "loss : 2.2157342433929443\n",
      "loss : 2.3329217433929443\n",
      "loss : 2.260709524154663\n",
      "loss : 2.3235023021698\n",
      "loss : 2.274338483810425\n",
      "loss : 2.2138240337371826\n",
      "loss : 2.260103225708008\n",
      "loss : 2.143970489501953\n",
      "loss : 2.2026565074920654\n",
      "loss : 2.2823355197906494\n",
      "loss : 2.216097593307495\n",
      "loss : 2.262993335723877\n",
      "loss : 2.2551214694976807\n",
      "loss : 2.1291205883026123\n",
      "loss : 2.281257390975952\n",
      "loss : 2.1474642753601074\n",
      "loss : 2.223684787750244\n",
      "loss : 2.291494846343994\n",
      "loss : 2.235598564147949\n",
      "loss : 2.101503372192383\n",
      "loss : 2.299623489379883\n",
      "loss : 2.1175880432128906\n",
      "loss : 2.2403883934020996\n",
      "loss : 2.1815907955169678\n",
      "loss : 2.19132661819458\n",
      "loss : 2.2403085231781006\n",
      "loss : 2.1650044918060303\n",
      "loss : 2.2352800369262695\n",
      "loss : 2.1977288722991943\n",
      "loss : 2.143904685974121\n",
      "loss : 2.1079671382904053\n",
      "loss : 2.292039155960083\n",
      "loss : 2.19206166267395\n",
      "loss : 2.147427558898926\n",
      "loss : 2.125757932662964\n",
      "loss : 2.1339306831359863\n",
      "loss : 2.1792869567871094\n",
      "loss : 2.160637140274048\n",
      "loss : 2.2313828468322754\n",
      "loss : 2.1048285961151123\n",
      "loss : 2.2425312995910645\n",
      "loss : 2.2072770595550537\n",
      "loss : 2.2094147205352783\n",
      "loss : 2.1625919342041016\n",
      "loss : 2.170736074447632\n",
      "loss : 2.2857444286346436\n",
      "loss : 2.2691500186920166\n",
      "loss : 2.2279860973358154\n",
      "loss : 2.2204067707061768\n",
      "loss : 2.2347614765167236\n",
      "loss : 2.1367952823638916\n",
      "loss : 2.198455810546875\n",
      "loss : 2.1558423042297363\n",
      "loss : 2.2677624225616455\n",
      "loss : 2.235426664352417\n",
      "loss : 2.2556254863739014\n",
      "loss : 2.199113607406616\n",
      "loss : 2.1610355377197266\n",
      "loss : 2.126809597015381\n",
      "loss : 2.2492401599884033\n",
      "loss : 2.265399932861328\n",
      "loss : 2.096900463104248\n",
      "loss : 2.1857669353485107\n",
      "loss : 2.2160568237304688\n",
      "loss : 2.2368979454040527\n",
      "loss : 2.1910743713378906\n",
      "loss : 2.0493147373199463\n",
      "loss : 2.058969497680664\n",
      "loss : 2.131747007369995\n",
      "loss : 2.0095720291137695\n",
      "loss : 2.1256558895111084\n",
      "loss : 2.194326400756836\n",
      "loss : 2.0394678115844727\n",
      "loss : 2.127358913421631\n",
      "loss : 2.2619566917419434\n",
      "loss : 2.126995801925659\n",
      "loss : 2.115206241607666\n",
      "loss : 2.192366123199463\n",
      "loss : 2.001568555831909\n",
      "loss : 2.176469087600708\n",
      "loss : 2.119800329208374\n",
      "loss : 2.1919591426849365\n",
      "loss : 2.095628023147583\n",
      "loss : 1.9259262084960938\n",
      "loss : 2.0993099212646484\n",
      "loss : 2.0117688179016113\n",
      "loss : 2.0041205883026123\n",
      "loss : 2.158843994140625\n",
      "loss : 2.165391445159912\n",
      "loss : 2.1945531368255615\n",
      "loss : 1.915355920791626\n",
      "loss : 2.117380142211914\n",
      "loss : 2.1014246940612793\n",
      "loss : 2.106076955795288\n",
      "loss : 2.1168570518493652\n",
      "loss : 2.141242265701294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 2.185746908187866\n",
      "loss : 2.0257627964019775\n",
      "loss : 2.059203624725342\n",
      "loss : 2.2017905712127686\n",
      "loss : 2.250516176223755\n",
      "loss : 2.1290485858917236\n",
      "loss : 2.250824451446533\n",
      "loss : 2.0988190174102783\n",
      "loss : 2.1555135250091553\n",
      "loss : 2.097372055053711\n",
      "loss : 2.0150437355041504\n",
      "loss : 2.111776113510132\n",
      "loss : 2.094329833984375\n",
      "loss : 2.0271918773651123\n",
      "loss : 2.1236143112182617\n",
      "loss : 2.243177652359009\n",
      "loss : 2.1897523403167725\n",
      "loss : 2.1100521087646484\n",
      "loss : 2.1240978240966797\n",
      "loss : 2.196443796157837\n",
      "loss : 2.0887317657470703\n",
      "loss : 1.9677135944366455\n",
      "loss : 2.0829861164093018\n",
      "loss : 2.0637261867523193\n",
      "loss : 2.1260006427764893\n",
      "loss : 2.0470921993255615\n",
      "loss : 2.1386637687683105\n",
      "loss : 1.9070701599121094\n",
      "loss : 1.9171069860458374\n",
      "loss : 2.1169440746307373\n",
      "loss : 2.1476047039031982\n",
      "loss : 2.109861135482788\n",
      "loss : 2.049635887145996\n",
      "loss : 1.9608360528945923\n",
      "loss : 1.979924201965332\n",
      "loss : 2.0695650577545166\n",
      "loss : 2.0271313190460205\n",
      "loss : 2.056251287460327\n",
      "loss : 2.160946846008301\n",
      "loss : 1.9922958612442017\n",
      "loss : 1.9433034658432007\n",
      "loss : 2.132560968399048\n",
      "loss : 2.0417659282684326\n",
      "loss : 2.1316702365875244\n",
      "loss : 2.1060643196105957\n",
      "loss : 2.0571134090423584\n",
      "loss : 2.0234248638153076\n",
      "loss : 2.059588670730591\n",
      "loss : 2.071078062057495\n",
      "loss : 1.9924334287643433\n",
      "loss : 2.0212814807891846\n",
      "loss : 2.012908697128296\n",
      "loss : 2.153237819671631\n",
      "loss : 2.084437608718872\n",
      "loss : 1.9641082286834717\n",
      "loss : 2.0329227447509766\n",
      "loss : 2.0329811573028564\n",
      "loss : 2.0610570907592773\n",
      "loss : 2.0305163860321045\n",
      "loss : 1.9937483072280884\n",
      "loss : 2.0337443351745605\n",
      "loss : 2.0574231147766113\n",
      "loss : 1.9566352367401123\n",
      "loss : 1.996850848197937\n",
      "loss : 2.046511650085449\n",
      "loss : 1.9174697399139404\n",
      "loss : 2.012847423553467\n",
      "loss : 2.1269781589508057\n",
      "loss : 2.0183534622192383\n",
      "loss : 1.972358226776123\n",
      "loss : 1.9650323390960693\n",
      "loss : 1.9955497980117798\n",
      "loss : 1.968543291091919\n",
      "loss : 1.9798932075500488\n",
      "loss : 2.031456470489502\n",
      "loss : 2.007754325866699\n",
      "loss : 1.987619400024414\n",
      "loss : 1.9305272102355957\n",
      "loss : 1.9099472761154175\n",
      "loss : 1.895259141921997\n",
      "loss : 1.9084783792495728\n",
      "loss : 1.9739861488342285\n",
      "loss : 1.8661772012710571\n",
      "loss : 1.98862886428833\n",
      "loss : 1.9894837141036987\n",
      "loss : 2.0228288173675537\n",
      "loss : 1.9678632020950317\n",
      "loss : 1.990829348564148\n",
      "loss : 1.9212654829025269\n",
      "loss : 1.8188152313232422\n",
      "loss : 1.941718578338623\n",
      "loss : 1.8375811576843262\n",
      "loss : 1.9475959539413452\n",
      "loss : 1.9925185441970825\n",
      "loss : 1.9325705766677856\n",
      "loss : 1.9804553985595703\n",
      "loss : 1.9894901514053345\n",
      "loss : 2.00189208984375\n",
      "loss : 1.8984425067901611\n",
      "loss : 1.9522931575775146\n",
      "loss : 1.9348779916763306\n",
      "loss : 1.892053484916687\n",
      "loss : 1.9314690828323364\n",
      "loss : 1.9417983293533325\n",
      "loss : 1.9988868236541748\n",
      "loss : 1.9449584484100342\n",
      "loss : 1.9313653707504272\n",
      "loss : 1.9295843839645386\n",
      "loss : 2.035135507583618\n",
      "loss : 1.9432586431503296\n",
      "loss : 1.9326852560043335\n",
      "loss : 1.9131531715393066\n",
      "loss : 1.861623764038086\n",
      "loss : 1.937930941581726\n",
      "loss : 1.909252643585205\n",
      "loss : 1.8425209522247314\n",
      "loss : 1.8862882852554321\n",
      "loss : 1.8259398937225342\n",
      "loss : 1.8707810640335083\n",
      "loss : 2.004880905151367\n",
      "loss : 1.8431396484375\n",
      "loss : 1.8858559131622314\n",
      "loss : 1.9922213554382324\n",
      "loss : 1.8924766778945923\n",
      "loss : 1.9598568677902222\n",
      "loss : 1.8828434944152832\n",
      "loss : 1.9670847654342651\n",
      "loss : 1.9607540369033813\n",
      "loss : 1.7818323373794556\n",
      "loss : 1.8779503107070923\n",
      "loss : 1.7657092809677124\n",
      "loss : 1.8538256883621216\n",
      "loss : 1.9058252573013306\n",
      "loss : 1.8738434314727783\n",
      "loss : 1.8978850841522217\n",
      "loss : 1.855681300163269\n",
      "loss : 1.8091989755630493\n",
      "loss : 1.8934707641601562\n",
      "loss : 1.8342092037200928\n",
      "loss : 1.7708793878555298\n",
      "loss : 1.841784954071045\n",
      "loss : 1.8633160591125488\n",
      "loss : 1.8559402227401733\n",
      "loss : 1.862923264503479\n",
      "loss : 1.8028842210769653\n",
      "loss : 1.8570303916931152\n",
      "loss : 1.8730454444885254\n",
      "loss : 1.8341126441955566\n",
      "loss : 1.8369089365005493\n",
      "loss : 1.7921669483184814\n",
      "loss : 1.8903285264968872\n",
      "loss : 1.6845322847366333\n",
      "loss : 1.9081475734710693\n",
      "loss : 1.959133267402649\n",
      "loss : 1.8694088459014893\n",
      "loss : 1.8360623121261597\n",
      "loss : 1.909149169921875\n",
      "loss : 1.8630813360214233\n",
      "loss : 1.8923351764678955\n",
      "loss : 1.840991497039795\n",
      "loss : 1.8913551568984985\n",
      "loss : 1.8233553171157837\n",
      "loss : 1.8184716701507568\n",
      "loss : 1.8727556467056274\n",
      "loss : 1.8570611476898193\n",
      "loss : 1.880403757095337\n",
      "loss : 1.7837529182434082\n",
      "loss : 1.7686452865600586\n",
      "loss : 1.9035643339157104\n",
      "loss : 1.8157594203948975\n",
      "loss : 1.8326326608657837\n",
      "loss : 1.7023166418075562\n",
      "loss : 1.8446011543273926\n",
      "loss : 1.8217779397964478\n",
      "loss : 1.8380011320114136\n",
      "loss : 1.7279974222183228\n",
      "loss : 1.7976244688034058\n",
      "loss : 1.770500659942627\n",
      "loss : 1.7568292617797852\n",
      "loss : 1.7941029071807861\n",
      "loss : 1.7401535511016846\n",
      "loss : 1.8674521446228027\n",
      "loss : 1.825666069984436\n",
      "loss : 1.7569581270217896\n",
      "loss : 1.8303290605545044\n",
      "loss : 1.8158777952194214\n",
      "loss : 1.7159340381622314\n",
      "loss : 1.874259114265442\n",
      "loss : 1.9136579036712646\n",
      "loss : 1.7078925371170044\n",
      "loss : 1.6257436275482178\n",
      "loss : 1.8202682733535767\n",
      "loss : 1.7604824304580688\n",
      "loss : 1.7769180536270142\n",
      "loss : 1.818859577178955\n",
      "loss : 1.8139930963516235\n",
      "loss : 1.7596163749694824\n",
      "loss : 1.760922908782959\n",
      "loss : 1.7433748245239258\n",
      "loss : 1.790195345878601\n",
      "loss : 1.6947282552719116\n",
      "loss : 1.823404312133789\n",
      "loss : 1.805534839630127\n",
      "loss : 1.8493050336837769\n",
      "loss : 1.7601754665374756\n",
      "loss : 1.8047215938568115\n",
      "loss : 1.8186980485916138\n",
      "loss : 1.851810336112976\n",
      "loss : 1.7682733535766602\n",
      "loss : 1.7358219623565674\n",
      "loss : 1.671082854270935\n",
      "loss : 1.7274131774902344\n",
      "loss : 1.75749933719635\n",
      "loss : 1.714233160018921\n",
      "loss : 1.7625099420547485\n",
      "loss : 1.734325647354126\n",
      "loss : 1.780988335609436\n",
      "loss : 1.7377151250839233\n",
      "loss : 1.7453793287277222\n",
      "loss : 1.7037936449050903\n",
      "loss : 1.764402985572815\n",
      "loss : 1.7272347211837769\n",
      "loss : 1.6997889280319214\n",
      "loss : 1.7627302408218384\n",
      "loss : 1.7404546737670898\n",
      "loss : 1.7931883335113525\n",
      "loss : 1.7478842735290527\n",
      "loss : 1.641751766204834\n",
      "loss : 1.7574901580810547\n",
      "loss : 1.8498733043670654\n",
      "loss : 1.6904810667037964\n",
      "loss : 1.7305631637573242\n",
      "loss : 1.658495545387268\n",
      "loss : 1.7294666767120361\n",
      "loss : 1.630579948425293\n",
      "loss : 1.687817931175232\n",
      "loss : 1.740315318107605\n",
      "loss : 1.6817744970321655\n",
      "loss : 1.748976469039917\n",
      "loss : 1.780043601989746\n",
      "loss : 1.7658487558364868\n",
      "loss : 1.6819090843200684\n",
      "loss : 1.7049936056137085\n",
      "loss : 1.7629512548446655\n",
      "loss : 1.7796663045883179\n",
      "loss : 1.7570767402648926\n",
      "loss : 1.7544039487838745\n",
      "loss : 1.6682312488555908\n",
      "loss : 1.6721009016036987\n",
      "loss : 1.7696256637573242\n",
      "loss : 1.7088018655776978\n",
      "loss : 1.698197603225708\n",
      "loss : 1.689697504043579\n",
      "loss : 1.7555261850357056\n",
      "loss : 1.781520962715149\n",
      "loss : 1.7497597932815552\n",
      "loss : 1.7834203243255615\n",
      "loss : 1.6456151008605957\n",
      "loss : 1.609222650527954\n",
      "loss : 1.7948718070983887\n",
      "loss : 1.7981690168380737\n",
      "loss : 1.7193796634674072\n",
      "loss : 1.7372965812683105\n",
      "loss : 1.6891518831253052\n",
      "loss : 1.7021887302398682\n",
      "loss : 1.7445427179336548\n",
      "loss : 1.678310751914978\n",
      "loss : 1.7781790494918823\n",
      "loss : 1.679718017578125\n",
      "loss : 1.657355546951294\n",
      "loss : 1.7194453477859497\n",
      "loss : 1.6446393728256226\n",
      "loss : 1.6750128269195557\n",
      "loss : 1.6396758556365967\n",
      "loss : 1.7323081493377686\n",
      "loss : 1.6935561895370483\n",
      "loss : 1.676124095916748\n",
      "loss : 1.5562763214111328\n",
      "loss : 1.5472146272659302\n",
      "loss : 1.658909559249878\n",
      "loss : 1.7241514921188354\n",
      "loss : 1.6880182027816772\n",
      "loss : 1.6414717435836792\n",
      "loss : 1.658754587173462\n",
      "loss : 1.6043062210083008\n",
      "loss : 1.69446861743927\n",
      "loss : 1.6554957628250122\n",
      "loss : 1.6296824216842651\n",
      "loss : 1.6660383939743042\n",
      "loss : 1.694428563117981\n",
      "loss : 1.7302584648132324\n",
      "loss : 1.7126023769378662\n",
      "loss : 1.7065373659133911\n",
      "loss : 1.5219907760620117\n",
      "loss : 1.586631417274475\n",
      "loss : 1.6829447746276855\n",
      "loss : 1.7126734256744385\n",
      "loss : 1.6253670454025269\n",
      "loss : 1.7721258401870728\n",
      "loss : 1.6678465604782104\n",
      "loss : 1.7020870447158813\n",
      "loss : 1.7060577869415283\n",
      "loss : 1.624046802520752\n",
      "loss : 1.6976356506347656\n",
      "loss : 1.6394398212432861\n",
      "loss : 1.6278799772262573\n",
      "loss : 1.6201733350753784\n",
      "loss : 1.6076892614364624\n",
      "loss : 1.677566647529602\n",
      "loss : 1.6924734115600586\n",
      "loss : 1.676048755645752\n",
      "loss : 1.5757156610488892\n",
      "loss : 1.6412543058395386\n",
      "loss : 1.6764767169952393\n",
      "loss : 1.6891162395477295\n",
      "loss : 1.5638700723648071\n",
      "loss : 1.5878912210464478\n",
      "loss : 1.6621670722961426\n",
      "loss : 1.7102738618850708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 1.5197137594223022\n",
      "loss : 1.580690622329712\n",
      "loss : 1.706526279449463\n",
      "loss : 1.5951104164123535\n",
      "loss : 1.6024038791656494\n",
      "loss : 1.5762357711791992\n",
      "loss : 1.6013884544372559\n",
      "loss : 1.663265585899353\n",
      "loss : 1.5583595037460327\n",
      "loss : 1.602799654006958\n",
      "loss : 1.5740516185760498\n",
      "loss : 1.6534732580184937\n",
      "loss : 1.6725341081619263\n",
      "loss : 1.6663711071014404\n",
      "loss : 1.5697890520095825\n",
      "loss : 1.6745049953460693\n",
      "loss : 1.5514754056930542\n",
      "loss : 1.6673332452774048\n",
      "loss : 1.677909255027771\n",
      "loss : 1.6372150182724\n",
      "loss : 1.6972520351409912\n",
      "loss : 1.6239728927612305\n",
      "loss : 1.6440584659576416\n",
      "loss : 1.5965166091918945\n",
      "loss : 1.6692332029342651\n",
      "loss : 1.607088565826416\n",
      "loss : 1.5839025974273682\n",
      "loss : 1.6329294443130493\n",
      "loss : 1.4539520740509033\n",
      "loss : 1.556962251663208\n",
      "loss : 1.4123847484588623\n",
      "loss : 1.659501314163208\n",
      "loss : 1.6794260740280151\n",
      "loss : 1.5399638414382935\n",
      "loss : 1.6710841655731201\n",
      "loss : 1.541279911994934\n",
      "loss : 1.6557152271270752\n",
      "loss : 1.677404761314392\n",
      "loss : 1.6775599718093872\n",
      "loss : 1.5766541957855225\n",
      "loss : 1.6405932903289795\n",
      "Step 1000 : Train Loss  1.4853, Val Loss 1.5779\n",
      "loss : 1.5841869115829468\n",
      "loss : 1.6146200895309448\n",
      "loss : 1.657828450202942\n",
      "loss : 1.682591438293457\n",
      "loss : 1.668062448501587\n",
      "loss : 1.6416120529174805\n",
      "loss : 1.5576601028442383\n",
      "loss : 1.5813862085342407\n",
      "loss : 1.5794621706008911\n",
      "loss : 1.5857566595077515\n",
      "loss : 1.478597640991211\n",
      "loss : 1.5977535247802734\n",
      "loss : 1.5427911281585693\n",
      "loss : 1.632652759552002\n",
      "loss : 1.610183596611023\n",
      "loss : 1.5797473192214966\n",
      "loss : 1.5122199058532715\n",
      "loss : 1.6252174377441406\n",
      "loss : 1.5347867012023926\n",
      "loss : 1.6401100158691406\n",
      "loss : 1.5815070867538452\n",
      "loss : 1.6155091524124146\n",
      "loss : 1.604272484779358\n",
      "loss : 1.5373106002807617\n",
      "loss : 1.6374088525772095\n",
      "loss : 1.5597983598709106\n",
      "loss : 1.5095419883728027\n",
      "loss : 1.6333082914352417\n",
      "loss : 1.6473913192749023\n",
      "loss : 1.6003003120422363\n",
      "loss : 1.6305714845657349\n",
      "loss : 1.4645127058029175\n",
      "loss : 1.6326260566711426\n",
      "loss : 1.5899207592010498\n",
      "loss : 1.6259042024612427\n",
      "loss : 1.6070175170898438\n",
      "loss : 1.53047513961792\n",
      "loss : 1.5809756517410278\n",
      "loss : 1.645993947982788\n",
      "loss : 1.5616943836212158\n",
      "loss : 1.652513027191162\n",
      "loss : 1.56419837474823\n",
      "loss : 1.539368987083435\n",
      "loss : 1.5912538766860962\n",
      "loss : 1.5224484205245972\n",
      "loss : 1.5468944311141968\n",
      "loss : 1.5197750329971313\n",
      "loss : 1.5792127847671509\n",
      "loss : 1.5288233757019043\n",
      "loss : 1.559505581855774\n",
      "loss : 1.568084478378296\n",
      "loss : 1.5477802753448486\n",
      "loss : 1.5630877017974854\n",
      "loss : 1.5099246501922607\n",
      "loss : 1.6086347103118896\n",
      "loss : 1.6212776899337769\n",
      "loss : 1.6476672887802124\n",
      "loss : 1.4978530406951904\n",
      "loss : 1.5277073383331299\n",
      "loss : 1.5255109071731567\n",
      "loss : 1.601808786392212\n",
      "loss : 1.6031757593154907\n",
      "loss : 1.5394195318222046\n",
      "loss : 1.5482778549194336\n",
      "loss : 1.534366488456726\n",
      "loss : 1.530312180519104\n",
      "loss : 1.5301175117492676\n",
      "loss : 1.6177836656570435\n",
      "loss : 1.5707734823226929\n",
      "loss : 1.5245788097381592\n",
      "loss : 1.4333206415176392\n",
      "loss : 1.6144765615463257\n",
      "loss : 1.5169787406921387\n",
      "loss : 1.5456767082214355\n",
      "loss : 1.5175118446350098\n",
      "loss : 1.5052368640899658\n",
      "loss : 1.5348140001296997\n",
      "loss : 1.4703272581100464\n",
      "loss : 1.6114801168441772\n",
      "loss : 1.5144178867340088\n",
      "loss : 1.619277000427246\n",
      "loss : 1.5841097831726074\n",
      "loss : 1.4138132333755493\n",
      "loss : 1.4509696960449219\n",
      "loss : 1.5707216262817383\n",
      "loss : 1.486320972442627\n",
      "loss : 1.501781940460205\n",
      "loss : 1.486878514289856\n",
      "loss : 1.5038177967071533\n",
      "loss : 1.5134466886520386\n",
      "loss : 1.4958337545394897\n",
      "loss : 1.576822280883789\n",
      "loss : 1.567466139793396\n",
      "loss : 1.5641919374465942\n",
      "loss : 1.6141085624694824\n",
      "loss : 1.5670908689498901\n",
      "loss : 1.4942196607589722\n",
      "loss : 1.488121509552002\n",
      "loss : 1.530036449432373\n",
      "loss : 1.5891152620315552\n",
      "loss : 1.58475923538208\n",
      "loss : 1.513152003288269\n",
      "loss : 1.6138503551483154\n",
      "loss : 1.4680085182189941\n",
      "loss : 1.444992184638977\n",
      "loss : 1.5733885765075684\n",
      "loss : 1.455653190612793\n",
      "loss : 1.5412718057632446\n",
      "loss : 1.5400104522705078\n",
      "loss : 1.5052781105041504\n",
      "loss : 1.451304316520691\n",
      "loss : 1.4479801654815674\n",
      "loss : 1.534295678138733\n",
      "loss : 1.4257673025131226\n",
      "loss : 1.4896882772445679\n",
      "loss : 1.5946248769760132\n",
      "loss : 1.440051794052124\n",
      "loss : 1.4182556867599487\n",
      "loss : 1.457657814025879\n",
      "loss : 1.4964056015014648\n",
      "loss : 1.5568925142288208\n",
      "loss : 1.53914475440979\n",
      "loss : 1.5381910800933838\n",
      "loss : 1.549789309501648\n",
      "loss : 1.5540733337402344\n",
      "loss : 1.4503408670425415\n",
      "loss : 1.5391645431518555\n",
      "loss : 1.4837738275527954\n",
      "loss : 1.458809733390808\n",
      "loss : 1.4390511512756348\n",
      "loss : 1.5257563591003418\n",
      "loss : 1.566365361213684\n",
      "loss : 1.5055196285247803\n",
      "loss : 1.4767489433288574\n",
      "loss : 1.5893590450286865\n",
      "loss : 1.4980852603912354\n",
      "loss : 1.4784846305847168\n",
      "loss : 1.4362164735794067\n",
      "loss : 1.5314815044403076\n",
      "loss : 1.4986881017684937\n",
      "loss : 1.497254729270935\n",
      "loss : 1.4315797090530396\n",
      "loss : 1.5484412908554077\n",
      "loss : 1.512847661972046\n",
      "loss : 1.530458688735962\n",
      "loss : 1.4517946243286133\n",
      "loss : 1.4644391536712646\n",
      "loss : 1.5601800680160522\n",
      "loss : 1.4983989000320435\n",
      "loss : 1.564666509628296\n",
      "loss : 1.5532501935958862\n",
      "loss : 1.4856163263320923\n",
      "loss : 1.4731428623199463\n",
      "loss : 1.502002477645874\n",
      "loss : 1.472926378250122\n",
      "loss : 1.5828157663345337\n",
      "loss : 1.4896217584609985\n",
      "loss : 1.5413330793380737\n",
      "loss : 1.481102705001831\n",
      "loss : 1.4649728536605835\n",
      "loss : 1.4997063875198364\n",
      "loss : 1.467225432395935\n",
      "loss : 1.5786514282226562\n",
      "loss : 1.4761559963226318\n",
      "loss : 1.4850338697433472\n",
      "loss : 1.4867624044418335\n",
      "loss : 1.462785005569458\n",
      "loss : 1.4616113901138306\n",
      "loss : 1.4649944305419922\n",
      "loss : 1.489586591720581\n",
      "loss : 1.5198099613189697\n",
      "loss : 1.5320024490356445\n",
      "loss : 1.4703353643417358\n",
      "loss : 1.524308443069458\n",
      "loss : 1.544221043586731\n",
      "loss : 1.5473425388336182\n",
      "loss : 1.4317272901535034\n",
      "loss : 1.4400967359542847\n",
      "loss : 1.478557825088501\n",
      "loss : 1.4782992601394653\n",
      "loss : 1.4706034660339355\n",
      "loss : 1.5090570449829102\n",
      "loss : 1.4359885454177856\n",
      "loss : 1.5392673015594482\n",
      "loss : 1.3901989459991455\n",
      "loss : 1.4112495183944702\n",
      "loss : 1.493574857711792\n",
      "loss : 1.4043097496032715\n",
      "loss : 1.4470264911651611\n",
      "loss : 1.4872262477874756\n",
      "loss : 1.4188224077224731\n",
      "loss : 1.4719226360321045\n",
      "loss : 1.4605515003204346\n",
      "loss : 1.4469459056854248\n",
      "loss : 1.4334017038345337\n",
      "loss : 1.4982305765151978\n",
      "loss : 1.4912306070327759\n",
      "loss : 1.4063639640808105\n",
      "loss : 1.4661900997161865\n",
      "loss : 1.4289486408233643\n",
      "loss : 1.5508160591125488\n",
      "loss : 1.4185919761657715\n",
      "loss : 1.5428987741470337\n",
      "loss : 1.5289899110794067\n",
      "loss : 1.5045745372772217\n",
      "loss : 1.4865878820419312\n",
      "loss : 1.5663816928863525\n",
      "loss : 1.418944239616394\n",
      "loss : 1.4433335065841675\n",
      "loss : 1.465268850326538\n",
      "loss : 1.4581539630889893\n",
      "loss : 1.4597275257110596\n",
      "loss : 1.4587732553482056\n",
      "loss : 1.4274324178695679\n",
      "loss : 1.5063912868499756\n",
      "loss : 1.453094720840454\n",
      "loss : 1.3894058465957642\n",
      "loss : 1.4791289567947388\n",
      "loss : 1.4356096982955933\n",
      "loss : 1.4658277034759521\n",
      "loss : 1.507380485534668\n",
      "loss : 1.476057529449463\n",
      "loss : 1.4953066110610962\n",
      "loss : 1.4503724575042725\n",
      "loss : 1.4667550325393677\n",
      "loss : 1.4925620555877686\n",
      "loss : 1.5503257513046265\n",
      "loss : 1.3871948719024658\n",
      "loss : 1.4888161420822144\n",
      "loss : 1.5340204238891602\n",
      "loss : 1.4721827507019043\n",
      "loss : 1.4790436029434204\n",
      "loss : 1.4364327192306519\n",
      "loss : 1.4649200439453125\n",
      "loss : 1.4627326726913452\n",
      "loss : 1.4774869680404663\n",
      "loss : 1.5313154458999634\n",
      "loss : 1.4683414697647095\n",
      "loss : 1.5068013668060303\n",
      "loss : 1.3550606966018677\n",
      "loss : 1.418647289276123\n",
      "loss : 1.3703103065490723\n",
      "loss : 1.450088620185852\n",
      "loss : 1.4666321277618408\n",
      "loss : 1.466107726097107\n",
      "loss : 1.527312159538269\n",
      "loss : 1.4360705614089966\n",
      "loss : 1.4769113063812256\n",
      "loss : 1.3821805715560913\n",
      "loss : 1.5115420818328857\n",
      "loss : 1.4763532876968384\n",
      "loss : 1.4410454034805298\n",
      "loss : 1.4409680366516113\n",
      "loss : 1.3625710010528564\n",
      "loss : 1.466872215270996\n",
      "loss : 1.4211289882659912\n",
      "loss : 1.416327953338623\n",
      "loss : 1.4024596214294434\n",
      "loss : 1.4827944040298462\n",
      "loss : 1.2939280271530151\n",
      "loss : 1.462775468826294\n",
      "loss : 1.4203859567642212\n",
      "loss : 1.4468756914138794\n",
      "loss : 1.4733580350875854\n",
      "loss : 1.4398952722549438\n",
      "loss : 1.4552361965179443\n",
      "loss : 1.4009555578231812\n",
      "loss : 1.5179688930511475\n",
      "loss : 1.3539477586746216\n",
      "loss : 1.4711487293243408\n",
      "loss : 1.4293212890625\n",
      "loss : 1.450948715209961\n",
      "loss : 1.4781510829925537\n",
      "loss : 1.4054954051971436\n",
      "loss : 1.4650280475616455\n",
      "loss : 1.3787212371826172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 1.397759199142456\n",
      "loss : 1.4017058610916138\n",
      "loss : 1.4202641248703003\n",
      "loss : 1.3330955505371094\n",
      "loss : 1.3763505220413208\n",
      "loss : 1.3693838119506836\n",
      "loss : 1.476276159286499\n",
      "loss : 1.3672266006469727\n",
      "loss : 1.433622121810913\n",
      "loss : 1.4135396480560303\n",
      "loss : 1.473800539970398\n",
      "loss : 1.495715856552124\n",
      "loss : 1.4577780961990356\n",
      "loss : 1.4562187194824219\n",
      "loss : 1.5070117712020874\n",
      "loss : 1.500606656074524\n",
      "loss : 1.387931227684021\n",
      "loss : 1.4562638998031616\n",
      "loss : 1.396591067314148\n",
      "loss : 1.433739185333252\n",
      "loss : 1.429668664932251\n",
      "loss : 1.4104597568511963\n",
      "loss : 1.450168251991272\n",
      "loss : 1.485832691192627\n",
      "loss : 1.4642728567123413\n",
      "loss : 1.4046478271484375\n",
      "loss : 1.3547717332839966\n",
      "loss : 1.391914963722229\n",
      "loss : 1.402285099029541\n",
      "loss : 1.3998130559921265\n",
      "loss : 1.3922338485717773\n",
      "loss : 1.4158837795257568\n",
      "loss : 1.4627273082733154\n",
      "loss : 1.3608320951461792\n",
      "loss : 1.4119353294372559\n",
      "loss : 1.4556756019592285\n",
      "loss : 1.4028375148773193\n",
      "loss : 1.4956254959106445\n",
      "loss : 1.4505131244659424\n",
      "loss : 1.3451741933822632\n",
      "loss : 1.3469069004058838\n",
      "loss : 1.469634771347046\n",
      "loss : 1.3517723083496094\n",
      "loss : 1.3759092092514038\n",
      "loss : 1.498839020729065\n",
      "loss : 1.3859232664108276\n",
      "loss : 1.4723023176193237\n",
      "loss : 1.4412420988082886\n",
      "loss : 1.4689605236053467\n",
      "loss : 1.477448582649231\n",
      "loss : 1.335999846458435\n",
      "loss : 1.4501954317092896\n",
      "loss : 1.4619652032852173\n",
      "loss : 1.3970428705215454\n",
      "loss : 1.4098788499832153\n",
      "loss : 1.4315297603607178\n",
      "loss : 1.2961490154266357\n",
      "loss : 1.4115452766418457\n",
      "loss : 1.3942770957946777\n",
      "loss : 1.422715425491333\n",
      "loss : 1.4489785432815552\n",
      "loss : 1.370015263557434\n",
      "loss : 1.4551060199737549\n",
      "loss : 1.3252716064453125\n",
      "loss : 1.427952766418457\n",
      "loss : 1.434356689453125\n",
      "loss : 1.3759126663208008\n",
      "loss : 1.4969967603683472\n",
      "loss : 1.3832181692123413\n",
      "loss : 1.3811655044555664\n",
      "loss : 1.4202302694320679\n",
      "loss : 1.3753435611724854\n",
      "loss : 1.3864392042160034\n",
      "loss : 1.432576298713684\n",
      "loss : 1.394544243812561\n",
      "loss : 1.4413708448410034\n",
      "loss : 1.378243088722229\n",
      "loss : 1.4200866222381592\n",
      "loss : 1.4098776578903198\n",
      "loss : 1.3924914598464966\n",
      "loss : 1.403865933418274\n",
      "loss : 1.3586076498031616\n",
      "loss : 1.3274179697036743\n",
      "loss : 1.4661873579025269\n",
      "loss : 1.4520814418792725\n",
      "loss : 1.4806557893753052\n",
      "loss : 1.4225423336029053\n",
      "loss : 1.443814992904663\n",
      "loss : 1.3629093170166016\n",
      "loss : 1.300711989402771\n",
      "loss : 1.3551833629608154\n",
      "loss : 1.3676269054412842\n",
      "loss : 1.4081331491470337\n",
      "loss : 1.3069326877593994\n",
      "loss : 1.443926453590393\n",
      "loss : 1.385558843612671\n",
      "loss : 1.412388563156128\n",
      "loss : 1.4156241416931152\n",
      "loss : 1.4036005735397339\n",
      "loss : 1.456226110458374\n",
      "loss : 1.4694336652755737\n",
      "loss : 1.360993504524231\n",
      "loss : 1.4455102682113647\n",
      "loss : 1.401341438293457\n",
      "loss : 1.3724325895309448\n",
      "loss : 1.3780368566513062\n",
      "loss : 1.3181747198104858\n",
      "loss : 1.352188229560852\n",
      "loss : 1.4556082487106323\n",
      "loss : 1.3828659057617188\n",
      "loss : 1.3591744899749756\n",
      "loss : 1.3881218433380127\n",
      "loss : 1.4358590841293335\n",
      "loss : 1.4717941284179688\n",
      "loss : 1.4012304544448853\n",
      "loss : 1.3761605024337769\n",
      "loss : 1.3354744911193848\n",
      "loss : 1.365127444267273\n",
      "loss : 1.2446929216384888\n",
      "loss : 1.3491586446762085\n",
      "loss : 1.3905078172683716\n",
      "loss : 1.3192980289459229\n",
      "loss : 1.4237548112869263\n",
      "loss : 1.3390862941741943\n",
      "loss : 1.3380626440048218\n",
      "loss : 1.3784193992614746\n",
      "loss : 1.3160736560821533\n",
      "loss : 1.439415693283081\n",
      "loss : 1.392845630645752\n",
      "loss : 1.3762683868408203\n",
      "loss : 1.3535689115524292\n",
      "loss : 1.4089032411575317\n",
      "loss : 1.3627097606658936\n",
      "loss : 1.3685309886932373\n",
      "loss : 1.4281971454620361\n",
      "loss : 1.420080542564392\n",
      "loss : 1.3758796453475952\n",
      "loss : 1.3791033029556274\n",
      "loss : 1.3406596183776855\n",
      "loss : 1.3461939096450806\n",
      "loss : 1.4654067754745483\n",
      "loss : 1.4339779615402222\n",
      "loss : 1.4206206798553467\n",
      "loss : 1.4300422668457031\n",
      "loss : 1.4118504524230957\n",
      "loss : 1.3723071813583374\n",
      "loss : 1.3695753812789917\n",
      "loss : 1.303829312324524\n",
      "loss : 1.3081555366516113\n",
      "loss : 1.3697770833969116\n",
      "loss : 1.4362894296646118\n",
      "loss : 1.4368171691894531\n",
      "loss : 1.3669036626815796\n",
      "loss : 1.3115102052688599\n",
      "loss : 1.3661226034164429\n",
      "loss : 1.4630619287490845\n",
      "loss : 1.39937162399292\n",
      "loss : 1.315233826637268\n",
      "loss : 1.3354729413986206\n",
      "loss : 1.4111030101776123\n",
      "loss : 1.3002043962478638\n",
      "loss : 1.368575096130371\n",
      "loss : 1.3746751546859741\n",
      "loss : 1.3280879259109497\n",
      "loss : 1.3340996503829956\n",
      "loss : 1.3171442747116089\n",
      "loss : 1.3946720361709595\n",
      "loss : 1.3801603317260742\n",
      "loss : 1.4267683029174805\n",
      "loss : 1.3648130893707275\n",
      "loss : 1.3504672050476074\n",
      "loss : 1.3614063262939453\n",
      "loss : 1.4020564556121826\n",
      "loss : 1.4029520750045776\n",
      "loss : 1.3322336673736572\n",
      "loss : 1.4211945533752441\n",
      "loss : 1.3786225318908691\n",
      "loss : 1.4182252883911133\n",
      "loss : 1.3913254737854004\n",
      "loss : 1.3442468643188477\n",
      "loss : 1.2701199054718018\n",
      "loss : 1.3852704763412476\n",
      "loss : 1.383954644203186\n",
      "loss : 1.3127082586288452\n",
      "loss : 1.4069366455078125\n",
      "loss : 1.4094189405441284\n",
      "loss : 1.3276699781417847\n",
      "loss : 1.4353159666061401\n",
      "loss : 1.3188879489898682\n",
      "loss : 1.383475661277771\n",
      "loss : 1.428884744644165\n",
      "loss : 1.3459185361862183\n",
      "loss : 1.3850325345993042\n",
      "loss : 1.3979287147521973\n",
      "loss : 1.3800485134124756\n",
      "loss : 1.3400226831436157\n",
      "loss : 1.4128882884979248\n",
      "loss : 1.4129161834716797\n",
      "loss : 1.3915711641311646\n",
      "loss : 1.377148985862732\n",
      "loss : 1.355819821357727\n",
      "loss : 1.314964771270752\n",
      "loss : 1.2533591985702515\n",
      "loss : 1.3449745178222656\n",
      "loss : 1.3421226739883423\n",
      "loss : 1.3752003908157349\n",
      "loss : 1.3451370000839233\n",
      "loss : 1.2482459545135498\n",
      "loss : 1.3005599975585938\n",
      "loss : 1.361499547958374\n",
      "loss : 1.3644285202026367\n",
      "loss : 1.3911195993423462\n",
      "loss : 1.3469499349594116\n",
      "loss : 1.2799447774887085\n",
      "loss : 1.3932057619094849\n",
      "loss : 1.4352561235427856\n",
      "loss : 1.3629709482192993\n",
      "loss : 1.3147119283676147\n",
      "loss : 1.3270456790924072\n",
      "loss : 1.3416194915771484\n",
      "loss : 1.2461048364639282\n",
      "loss : 1.4108487367630005\n",
      "loss : 1.3002877235412598\n",
      "loss : 1.4068098068237305\n",
      "Step 1500 : Train Loss  1.2418, Val Loss 1.4104\n",
      "loss : 1.309788703918457\n",
      "loss : 1.387577772140503\n",
      "loss : 1.3090096712112427\n",
      "loss : 1.4206113815307617\n",
      "loss : 1.3625450134277344\n",
      "loss : 1.3769882917404175\n",
      "loss : 1.4309313297271729\n",
      "loss : 1.366146445274353\n",
      "loss : 1.3311296701431274\n",
      "loss : 1.2737005949020386\n",
      "loss : 1.354279637336731\n",
      "loss : 1.3979275226593018\n",
      "loss : 1.305108904838562\n",
      "loss : 1.3208445310592651\n",
      "loss : 1.366933822631836\n",
      "loss : 1.3695285320281982\n",
      "loss : 1.3204314708709717\n",
      "loss : 1.2685503959655762\n",
      "loss : 1.4060471057891846\n",
      "loss : 1.3345024585723877\n",
      "loss : 1.3822436332702637\n",
      "loss : 1.3575445413589478\n",
      "loss : 1.3152555227279663\n",
      "loss : 1.326415777206421\n",
      "loss : 1.4295003414154053\n",
      "loss : 1.3713276386260986\n",
      "loss : 1.3427129983901978\n",
      "loss : 1.2741260528564453\n",
      "loss : 1.3196055889129639\n",
      "loss : 1.2685178518295288\n",
      "loss : 1.4289968013763428\n",
      "loss : 1.2981356382369995\n",
      "loss : 1.4133031368255615\n",
      "loss : 1.3427098989486694\n",
      "loss : 1.3750560283660889\n",
      "loss : 1.3526045083999634\n",
      "loss : 1.3147449493408203\n",
      "loss : 1.301697015762329\n",
      "loss : 1.3889813423156738\n",
      "loss : 1.221226453781128\n",
      "loss : 1.3362700939178467\n",
      "loss : 1.3802951574325562\n",
      "loss : 1.4379879236221313\n",
      "loss : 1.3417192697525024\n",
      "loss : 1.3974519968032837\n",
      "loss : 1.381899118423462\n",
      "loss : 1.328433871269226\n",
      "loss : 1.3384201526641846\n",
      "loss : 1.2665809392929077\n",
      "loss : 1.333504557609558\n",
      "loss : 1.4223721027374268\n",
      "loss : 1.3917957544326782\n",
      "loss : 1.291853666305542\n",
      "loss : 1.370485544204712\n",
      "loss : 1.3806346654891968\n",
      "loss : 1.3541666269302368\n",
      "loss : 1.312286376953125\n",
      "loss : 1.3435479402542114\n",
      "loss : 1.3133856058120728\n",
      "loss : 1.2933827638626099\n",
      "loss : 1.2886543273925781\n",
      "loss : 1.3732134103775024\n",
      "loss : 1.314587950706482\n",
      "loss : 1.3279666900634766\n",
      "loss : 1.3623061180114746\n",
      "loss : 1.3170255422592163\n",
      "loss : 1.3233768939971924\n",
      "loss : 1.2379789352416992\n",
      "loss : 1.3218252658843994\n",
      "loss : 1.3648998737335205\n",
      "loss : 1.2810566425323486\n",
      "loss : 1.3094784021377563\n",
      "loss : 1.2370918989181519\n",
      "loss : 1.3151187896728516\n",
      "loss : 1.2847747802734375\n",
      "loss : 1.3169647455215454\n",
      "loss : 1.3392603397369385\n",
      "loss : 1.2533891201019287\n",
      "loss : 1.3109986782073975\n",
      "loss : 1.316575288772583\n",
      "loss : 1.2581933736801147\n",
      "loss : 1.358907699584961\n",
      "loss : 1.3435355424880981\n",
      "loss : 1.2948580980300903\n",
      "loss : 1.2809603214263916\n",
      "loss : 1.2927093505859375\n",
      "loss : 1.3959331512451172\n",
      "loss : 1.388724684715271\n",
      "loss : 1.3239458799362183\n",
      "loss : 1.3780462741851807\n",
      "loss : 1.3317570686340332\n",
      "loss : 1.269215703010559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 1.321151614189148\n",
      "loss : 1.3541505336761475\n",
      "loss : 1.3186520338058472\n",
      "loss : 1.3249645233154297\n",
      "loss : 1.3963241577148438\n",
      "loss : 1.3441747426986694\n",
      "loss : 1.324652075767517\n",
      "loss : 1.2839033603668213\n",
      "loss : 1.3348767757415771\n",
      "loss : 1.3264321088790894\n",
      "loss : 1.3078577518463135\n",
      "loss : 1.351017713546753\n",
      "loss : 1.3535723686218262\n",
      "loss : 1.3919739723205566\n",
      "loss : 1.2412229776382446\n",
      "loss : 1.4145535230636597\n",
      "loss : 1.3673105239868164\n",
      "loss : 1.3477081060409546\n",
      "loss : 1.3316664695739746\n",
      "loss : 1.2828069925308228\n",
      "loss : 1.3315277099609375\n",
      "loss : 1.296713948249817\n",
      "loss : 1.3693233728408813\n",
      "loss : 1.3572561740875244\n",
      "loss : 1.2950165271759033\n",
      "loss : 1.312694787979126\n",
      "loss : 1.2700657844543457\n",
      "loss : 1.3447580337524414\n",
      "loss : 1.36197030544281\n",
      "loss : 1.291976809501648\n",
      "loss : 1.3184733390808105\n",
      "loss : 1.3224384784698486\n",
      "loss : 1.3668339252471924\n",
      "loss : 1.3676879405975342\n",
      "loss : 1.3158369064331055\n",
      "loss : 1.3434418439865112\n",
      "loss : 1.3720698356628418\n",
      "loss : 1.219823956489563\n",
      "loss : 1.2380332946777344\n",
      "loss : 1.292839765548706\n",
      "loss : 1.2788524627685547\n",
      "loss : 1.3127398490905762\n",
      "loss : 1.301324486732483\n",
      "loss : 1.360404372215271\n",
      "loss : 1.2888765335083008\n",
      "loss : 1.3802915811538696\n",
      "loss : 1.3563423156738281\n",
      "loss : 1.2975459098815918\n",
      "loss : 1.3596346378326416\n",
      "loss : 1.2792770862579346\n",
      "loss : 1.3978004455566406\n",
      "loss : 1.3597291707992554\n",
      "loss : 1.313962697982788\n",
      "loss : 1.3578596115112305\n",
      "loss : 1.3393148183822632\n",
      "loss : 1.3486158847808838\n",
      "loss : 1.3527840375900269\n",
      "loss : 1.32964289188385\n",
      "loss : 1.2924814224243164\n",
      "loss : 1.3430049419403076\n",
      "loss : 1.3287869691848755\n",
      "loss : 1.3616808652877808\n",
      "loss : 1.3228124380111694\n",
      "loss : 1.2912476062774658\n",
      "loss : 1.3359922170639038\n",
      "loss : 1.2565560340881348\n",
      "loss : 1.3618569374084473\n",
      "loss : 1.3728852272033691\n",
      "loss : 1.4282658100128174\n",
      "loss : 1.3859769105911255\n",
      "loss : 1.3359366655349731\n",
      "loss : 1.2850362062454224\n",
      "loss : 1.2449320554733276\n",
      "loss : 1.2877651453018188\n",
      "loss : 1.4009149074554443\n",
      "loss : 1.3525482416152954\n",
      "loss : 1.3329460620880127\n",
      "loss : 1.2733515501022339\n",
      "loss : 1.3878662586212158\n",
      "loss : 1.3568617105484009\n",
      "loss : 1.2596156597137451\n",
      "loss : 1.23008131980896\n",
      "loss : 1.3666130304336548\n",
      "loss : 1.3105965852737427\n",
      "loss : 1.2493102550506592\n",
      "loss : 1.2416926622390747\n",
      "loss : 1.3088181018829346\n",
      "loss : 1.2953718900680542\n",
      "loss : 1.3092485666275024\n",
      "loss : 1.2870383262634277\n",
      "loss : 1.358350396156311\n",
      "loss : 1.298126459121704\n",
      "loss : 1.236555814743042\n",
      "loss : 1.2721374034881592\n",
      "loss : 1.2250701189041138\n",
      "loss : 1.2975330352783203\n",
      "loss : 1.286884069442749\n",
      "loss : 1.360809087753296\n",
      "loss : 1.2406295537948608\n",
      "loss : 1.3042771816253662\n",
      "loss : 1.382246732711792\n",
      "loss : 1.2835475206375122\n",
      "loss : 1.3667618036270142\n",
      "loss : 1.250192403793335\n",
      "loss : 1.3259077072143555\n",
      "loss : 1.3069415092468262\n",
      "loss : 1.3319764137268066\n",
      "loss : 1.2361639738082886\n",
      "loss : 1.369284987449646\n",
      "loss : 1.3611944913864136\n",
      "loss : 1.310889720916748\n",
      "loss : 1.318834900856018\n",
      "loss : 1.307701587677002\n",
      "loss : 1.3135136365890503\n",
      "loss : 1.3215508460998535\n",
      "loss : 1.3112125396728516\n",
      "loss : 1.2777639627456665\n",
      "loss : 1.2783522605895996\n",
      "loss : 1.2145717144012451\n",
      "loss : 1.2272323369979858\n",
      "loss : 1.3288010358810425\n",
      "loss : 1.3010046482086182\n",
      "loss : 1.2758733034133911\n",
      "loss : 1.295455813407898\n",
      "loss : 1.311862587928772\n",
      "loss : 1.2725199460983276\n",
      "loss : 1.3323417901992798\n",
      "loss : 1.2562536001205444\n",
      "loss : 1.3036142587661743\n",
      "loss : 1.312118649482727\n",
      "loss : 1.3428168296813965\n",
      "loss : 1.302674651145935\n",
      "loss : 1.268760323524475\n",
      "loss : 1.3120379447937012\n",
      "loss : 1.3111512660980225\n",
      "loss : 1.2765141725540161\n",
      "loss : 1.3014283180236816\n",
      "loss : 1.251322865486145\n",
      "loss : 1.2664728164672852\n",
      "loss : 1.291191577911377\n",
      "loss : 1.3602871894836426\n",
      "loss : 1.2633553743362427\n",
      "loss : 1.3262414932250977\n",
      "loss : 1.239990234375\n",
      "loss : 1.3079607486724854\n",
      "loss : 1.1953086853027344\n",
      "loss : 1.312151312828064\n",
      "loss : 1.3107142448425293\n",
      "loss : 1.2713409662246704\n",
      "loss : 1.264084815979004\n",
      "loss : 1.3185888528823853\n",
      "loss : 1.2128384113311768\n",
      "loss : 1.3203284740447998\n",
      "loss : 1.3030186891555786\n",
      "loss : 1.3684731721878052\n",
      "loss : 1.3136987686157227\n",
      "loss : 1.329906940460205\n",
      "loss : 1.2762898206710815\n",
      "loss : 1.2594821453094482\n",
      "loss : 1.2631078958511353\n",
      "loss : 1.315828800201416\n",
      "loss : 1.3222286701202393\n",
      "loss : 1.3229575157165527\n",
      "loss : 1.2712998390197754\n",
      "loss : 1.276960015296936\n",
      "loss : 1.3429112434387207\n",
      "loss : 1.2259507179260254\n",
      "loss : 1.3653538227081299\n",
      "loss : 1.2576684951782227\n",
      "loss : 1.326219916343689\n",
      "loss : 1.2254347801208496\n",
      "loss : 1.2587780952453613\n",
      "loss : 1.274765968322754\n",
      "loss : 1.3375695943832397\n",
      "loss : 1.3263322114944458\n",
      "loss : 1.2810670137405396\n",
      "loss : 1.278275966644287\n",
      "loss : 1.2834635972976685\n",
      "loss : 1.2460383176803589\n",
      "loss : 1.279195785522461\n",
      "loss : 1.2514647245407104\n",
      "loss : 1.1927735805511475\n",
      "loss : 1.2322239875793457\n",
      "loss : 1.2584834098815918\n",
      "loss : 1.2058318853378296\n",
      "loss : 1.2951240539550781\n",
      "loss : 1.2975398302078247\n",
      "loss : 1.2787920236587524\n",
      "loss : 1.3008686304092407\n",
      "loss : 1.2989277839660645\n",
      "loss : 1.2086849212646484\n",
      "loss : 1.3365051746368408\n",
      "loss : 1.2321985960006714\n",
      "loss : 1.3284777402877808\n",
      "loss : 1.2399349212646484\n",
      "loss : 1.2389858961105347\n",
      "loss : 1.2930935621261597\n",
      "loss : 1.2199106216430664\n",
      "loss : 1.3008158206939697\n",
      "loss : 1.2241251468658447\n",
      "loss : 1.322623372077942\n",
      "loss : 1.3057150840759277\n",
      "loss : 1.299825668334961\n",
      "loss : 1.2538727521896362\n",
      "loss : 1.2057733535766602\n",
      "loss : 1.2155451774597168\n",
      "loss : 1.2576394081115723\n",
      "loss : 1.3040214776992798\n",
      "loss : 1.3060489892959595\n",
      "loss : 1.2351701259613037\n",
      "loss : 1.2810195684432983\n",
      "loss : 1.236451506614685\n",
      "loss : 1.231765627861023\n",
      "loss : 1.291877031326294\n",
      "loss : 1.3081636428833008\n",
      "loss : 1.3247076272964478\n",
      "loss : 1.2731902599334717\n",
      "loss : 1.2857139110565186\n",
      "loss : 1.2408785820007324\n",
      "loss : 1.2843824625015259\n",
      "loss : 1.255350947380066\n",
      "loss : 1.2520147562026978\n",
      "loss : 1.2338742017745972\n",
      "loss : 1.2690010070800781\n",
      "loss : 1.227433204650879\n",
      "loss : 1.2886362075805664\n",
      "loss : 1.2170144319534302\n",
      "loss : 1.2769875526428223\n",
      "loss : 1.2483805418014526\n",
      "loss : 1.3194146156311035\n",
      "loss : 1.2173367738723755\n",
      "loss : 1.2595709562301636\n",
      "loss : 1.2618873119354248\n",
      "loss : 1.1955047845840454\n",
      "loss : 1.2543613910675049\n",
      "loss : 1.2629436254501343\n",
      "loss : 1.2926414012908936\n",
      "loss : 1.2662864923477173\n",
      "loss : 1.2878421545028687\n",
      "loss : 1.1864476203918457\n",
      "loss : 1.3729711771011353\n",
      "loss : 1.2151812314987183\n",
      "loss : 1.260050654411316\n",
      "loss : 1.2446708679199219\n",
      "loss : 1.2280644178390503\n",
      "loss : 1.2701603174209595\n",
      "loss : 1.2442927360534668\n",
      "loss : 1.2014400959014893\n",
      "loss : 1.3152774572372437\n",
      "loss : 1.2718074321746826\n",
      "loss : 1.2968168258666992\n",
      "loss : 1.2775177955627441\n",
      "loss : 1.2606678009033203\n",
      "loss : 1.2306618690490723\n",
      "loss : 1.221448302268982\n",
      "loss : 1.3062790632247925\n",
      "loss : 1.2176986932754517\n",
      "loss : 1.284656047821045\n",
      "loss : 1.2792366743087769\n",
      "loss : 1.259200096130371\n",
      "loss : 1.2849637269973755\n",
      "loss : 1.2544153928756714\n",
      "loss : 1.2878679037094116\n",
      "loss : 1.2737689018249512\n",
      "loss : 1.269305944442749\n",
      "loss : 1.3006819486618042\n",
      "loss : 1.26023530960083\n",
      "loss : 1.2945178747177124\n",
      "loss : 1.3301570415496826\n",
      "loss : 1.241626501083374\n",
      "loss : 1.2666900157928467\n",
      "loss : 1.2380657196044922\n",
      "loss : 1.2160093784332275\n",
      "loss : 1.2950001955032349\n",
      "loss : 1.3032035827636719\n",
      "loss : 1.2295798063278198\n",
      "loss : 1.2200751304626465\n",
      "loss : 1.2223241329193115\n",
      "loss : 1.2534780502319336\n",
      "loss : 1.2447210550308228\n",
      "loss : 1.2747710943222046\n",
      "loss : 1.2840770483016968\n",
      "loss : 1.2202993631362915\n",
      "loss : 1.172841191291809\n",
      "loss : 1.26825749874115\n",
      "loss : 1.2976552248001099\n",
      "loss : 1.3114558458328247\n",
      "loss : 1.3386107683181763\n",
      "loss : 1.2556887865066528\n",
      "loss : 1.2285419702529907\n",
      "loss : 1.390009880065918\n",
      "loss : 1.2962809801101685\n",
      "loss : 1.1768560409545898\n",
      "loss : 1.2769224643707275\n",
      "loss : 1.2366834878921509\n",
      "loss : 1.2295359373092651\n",
      "loss : 1.3188453912734985\n",
      "loss : 1.2565486431121826\n",
      "loss : 1.221553087234497\n",
      "loss : 1.2140315771102905\n",
      "loss : 1.2846453189849854\n",
      "loss : 1.2890255451202393\n",
      "loss : 1.3031649589538574\n",
      "loss : 1.2816699743270874\n",
      "loss : 1.2109318971633911\n",
      "loss : 1.202593207359314\n",
      "loss : 1.2506318092346191\n",
      "loss : 1.2466237545013428\n",
      "loss : 1.2978813648223877\n",
      "loss : 1.2924846410751343\n",
      "loss : 1.2273842096328735\n",
      "loss : 1.1946090459823608\n",
      "loss : 1.2107231616973877\n",
      "loss : 1.1481996774673462\n",
      "loss : 1.2605112791061401\n",
      "loss : 1.2487919330596924\n",
      "loss : 1.3185551166534424\n",
      "loss : 1.2109392881393433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 1.3059706687927246\n",
      "loss : 1.250885248184204\n",
      "loss : 1.2000830173492432\n",
      "loss : 1.2180994749069214\n",
      "loss : 1.2545859813690186\n",
      "loss : 1.2920747995376587\n",
      "loss : 1.1569658517837524\n",
      "loss : 1.252584457397461\n",
      "loss : 1.3147879838943481\n",
      "loss : 1.2745342254638672\n",
      "loss : 1.266066312789917\n",
      "loss : 1.277614712715149\n",
      "loss : 1.2582751512527466\n",
      "loss : 1.2950770854949951\n",
      "loss : 1.2376466989517212\n",
      "loss : 1.285091519355774\n",
      "loss : 1.3233920335769653\n",
      "loss : 1.3100976943969727\n",
      "loss : 1.3492313623428345\n",
      "loss : 1.277780294418335\n",
      "loss : 1.2348591089248657\n",
      "loss : 1.2531218528747559\n",
      "loss : 1.2838404178619385\n",
      "loss : 1.2350252866744995\n",
      "loss : 1.2685632705688477\n",
      "loss : 1.2790088653564453\n",
      "loss : 1.3000984191894531\n",
      "loss : 1.2262601852416992\n",
      "loss : 1.2513234615325928\n",
      "loss : 1.2465778589248657\n",
      "loss : 1.2443115711212158\n",
      "loss : 1.2436277866363525\n",
      "loss : 1.211607575416565\n",
      "loss : 1.2171056270599365\n",
      "loss : 1.2637766599655151\n",
      "loss : 1.2902249097824097\n",
      "loss : 1.2671940326690674\n",
      "loss : 1.2871214151382446\n",
      "loss : 1.2288860082626343\n",
      "loss : 1.2936828136444092\n",
      "loss : 1.195765495300293\n",
      "loss : 1.1881086826324463\n",
      "loss : 1.2825416326522827\n",
      "loss : 1.270470142364502\n",
      "loss : 1.2595229148864746\n",
      "loss : 1.1921871900558472\n",
      "loss : 1.2678638696670532\n",
      "loss : 1.2008994817733765\n",
      "loss : 1.271884799003601\n",
      "loss : 1.2137471437454224\n",
      "loss : 1.2227338552474976\n",
      "loss : 1.2580724954605103\n",
      "loss : 1.2661211490631104\n",
      "loss : 1.2593159675598145\n",
      "loss : 1.1868972778320312\n",
      "loss : 1.27358877658844\n",
      "loss : 1.2593215703964233\n",
      "loss : 1.2660157680511475\n",
      "loss : 1.2376224994659424\n",
      "loss : 1.190952181816101\n",
      "loss : 1.274441123008728\n",
      "loss : 1.208237648010254\n",
      "loss : 1.208739161491394\n",
      "loss : 1.223939299583435\n",
      "loss : 1.188830852508545\n",
      "loss : 1.2463691234588623\n",
      "loss : 1.2636685371398926\n",
      "loss : 1.2391687631607056\n",
      "loss : 1.1764925718307495\n",
      "loss : 1.294686198234558\n",
      "loss : 1.2645620107650757\n",
      "loss : 1.2808955907821655\n",
      "loss : 1.232163906097412\n",
      "loss : 1.239760160446167\n",
      "loss : 1.2309982776641846\n",
      "loss : 1.253324031829834\n",
      "loss : 1.2555477619171143\n",
      "loss : 1.2282713651657104\n",
      "loss : 1.2296185493469238\n",
      "loss : 1.2947884798049927\n",
      "loss : 1.2203468084335327\n",
      "loss : 1.223332405090332\n",
      "loss : 1.2514621019363403\n",
      "loss : 1.2263847589492798\n",
      "loss : 1.2748383283615112\n",
      "loss : 1.2445465326309204\n",
      "loss : 1.320967435836792\n",
      "loss : 1.1978728771209717\n",
      "loss : 1.255070447921753\n",
      "loss : 1.2805242538452148\n",
      "Step 2000 : Train Loss  1.1204, Val Loss 1.3587\n",
      "loss : 1.183883786201477\n",
      "loss : 1.1869852542877197\n",
      "loss : 1.2844494581222534\n",
      "loss : 1.2203551530838013\n",
      "loss : 1.2542169094085693\n",
      "loss : 1.265066146850586\n",
      "loss : 1.267529845237732\n",
      "loss : 1.302919864654541\n",
      "loss : 1.2776042222976685\n",
      "loss : 1.2542318105697632\n",
      "loss : 1.2902884483337402\n",
      "loss : 1.177577257156372\n",
      "loss : 1.2361549139022827\n",
      "loss : 1.2653995752334595\n",
      "loss : 1.260329246520996\n",
      "loss : 1.1993577480316162\n",
      "loss : 1.2709654569625854\n",
      "loss : 1.3133904933929443\n",
      "loss : 1.2741501331329346\n",
      "loss : 1.2575830221176147\n",
      "loss : 1.2610807418823242\n",
      "loss : 1.236791729927063\n",
      "loss : 1.203734278678894\n",
      "loss : 1.271114706993103\n",
      "loss : 1.3023219108581543\n",
      "loss : 1.2552969455718994\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Evaluate loss\u001b[39;00m\n\u001b[0;32m     18\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model(xb, yb)\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Setting grads to None rather than 0 for memory efficiency\u001b[39;00m\n\u001b[0;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:858\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[1;34m(self, format_spec)\u001b[0m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, format_spec)\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[1;32m--> 858\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[0;32m    859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# Using AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for iteration in range(max_iters):\n",
    "    \n",
    "    # Periodically evaluate loss and once after all training iterations are done\n",
    "    if iteration % eval_interval == 0 or iteration == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step {iteration} : Train Loss  {losses['train']:.4f}, Val Loss {losses['val']:.4f}\")\n",
    "        \n",
    "    # sample o batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # Evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    print(f\"loss : {loss}\")\n",
    "    optimizer.zero_grad(set_to_none = True)  # Setting grads to None rather than 0 for memory efficiency\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d5b66aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tf med are free the more\n",
      "On ya I think he was just input offer 5 marks... So ig only you want improve to get to mat you seneshn\n",
      "Oh\n",
      "Dude\n",
      "Come areferristely cally on worth its just go to eg ound one?\n",
      "Those happth tomorrow\n",
      "Can you have?\n",
      "Well so for youbother 1^2..\n",
      "really latelect\n",
      "This is really than go to allies a lot who wont me in the know \n",
      "We get this her won't changes by this with you really are standing psifically create rt\n",
      "Yours retuive valuela la\n",
      "I can all be an lotta whan I mangered that\n",
      "But\n"
     ]
    }
   ],
   "source": [
    "# After training for roughly 13 minutes\n",
    "# Generate from the model\n",
    "context = torch.zeros((1, 1), dtype = torch.long, device = device)\n",
    "encoded_text = m.generate(context, max_new_tokens = 500)[0].tolist()\n",
    "print(decode(encoded_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fcb1c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "torch.save(m.state_dict(), './models/chat-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4523c869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTLanguageModel(\n",
       "  (token_embedding_table): Embedding(99, 384)\n",
       "  (position_embedding_table): Embedding(256, 384)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (self_attention): MultiHeadedAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (1): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (2): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (3): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (4): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (5): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed_forward_network): FeedForward(\n",
       "        (network): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (self_attention): MultiHeadedAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (1): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (2): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (3): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (4): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (5): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed_forward_network): FeedForward(\n",
       "        (network): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (self_attention): MultiHeadedAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (1): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (2): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (3): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (4): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (5): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed_forward_network): FeedForward(\n",
       "        (network): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (self_attention): MultiHeadedAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (1): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (2): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (3): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (4): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (5): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed_forward_network): FeedForward(\n",
       "        (network): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (self_attention): MultiHeadedAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (1): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (2): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (3): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (4): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (5): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed_forward_network): FeedForward(\n",
       "        (network): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (self_attention): MultiHeadedAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (1): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (2): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (3): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (4): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (5): Head(\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed_forward_network): FeedForward(\n",
       "        (network): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=384, out_features=99, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the model\n",
    "model = GPTLanguageModel()\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('./models/chat-model'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6228a3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(input_text):\n",
    "    encoded_context = torch.as_tensor(encode(input_text), dtype = torch.long, device = device)\n",
    "    encoded_context = torch.stack((encoded_context,)) # To turn it into a tensor of dimensions (1, context_length)\n",
    "    n = len(input_text)\n",
    "    encoded_text = model.generate(encoded_context, max_new_tokens = random.randint(20, 200))[0].tolist()\n",
    "    decoded_response = decode(encoded_text)[n:]\n",
    "    return decoded_response, len(decoded_response) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9966826b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You : Hey there, how's everything going?\n",
      "Response (104 characters) : \n",
      " It says came a good to mark slow\n",
      "So its a website?\n",
      "naah....I know how but the best...https://and every\n"
     ]
    }
   ],
   "source": [
    "input_text = input('You : ')\n",
    "response, size = generate_response(input_text)\n",
    "print(f'Response ({size} characters) : {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ac383a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
